#!/usr/bin/env python3

import argparse
import collections
import datetime
import fnmatch
import getpass
import io
import json
import logging
import operator
import os
import os.path
import pwd
import re
from subprocess import PIPE, Popen
import redis
import shutil
import signal
import socket
import sqlite3
import sys
import time
import traceback
import uuid

import requests

from saq.analysis.root import RootAnalysis
from saq.configuration import get_config
from sqlalchemy import and_, or_

from saq.configuration.config import get_config_value, get_config_value_as_int
from saq.constants import CONFIG_GUI, CONFIG_GUI_LISTEN_ADDRESS, CONFIG_GUI_LISTEN_PORT, F_FILE, F_SUSPECT_FILE, G_COMPANY_ID, G_COMPANY_NAME, G_ENCRYPTION_KEY, G_FORCED_ALERTS, G_OTHER_PROXIES, G_SAQ_NODE, REDIS_DB_FOR_DETECTION_A, REDIS_DB_FOR_DETECTION_B, VALID_OBSERVABLE_TYPES
from saq.database.model import Persistence, PersistenceSource, Remediation
from saq.database.pool import get_db
from saq.engine.engine_configuration import EngineConfiguration
from saq.engine.enums import EngineExecutionMode, EngineType
from saq.environment import g, g_boolean, g_int, g_obj, get_base_dir, initialize_environment
from saq.error.reporting import report_exception
from saq.remediation import REMEDIATION_STATUS_IN_PROGRESS, REMEDIATION_STATUS_NEW

parser = argparse.ArgumentParser(description="Analysis Correlation Engine")
parser.add_argument('--saq-home', required=False, dest='saq_home', default=None,
    help="Sets the base directory of ACE.")
parser.add_argument('-L', '--logging-config-path', required=False, dest='logging_config_path', default=None,
    help="Path to the logging configuration file.")
parser.add_argument('-c', '--config-path', required=False, dest='config_paths', action='append', default=[],
    help="""ACE configuration files. $SAQ_HOME/etc/saq.default.ini is always loaded, additional override default settings.
         This option can be specified multiple times and each file is loaded in order.""")
parser.add_argument('--log-level', required=False, dest='log_level', default=None,
    help="Change the root log level.")
parser.add_argument('-u', '--user-name', required=False, dest='user_name', default=None,
    help="The user name of the ACE user executing the command. This information is required for some commands.")
parser.add_argument('--start', required=False, dest='start', default=False, action='store_true',
    help="Start the specified service.  Blocks keyboard unless --daemon (-d) is used.")
parser.add_argument('--stop', required=False, dest='stop', default=False, action='store_true',
    help="Stop the specified service.  Only applies to services started with --daemon (-d).")
parser.add_argument('-d', '--daemon', required=False, dest='daemon', default=False, action='store_true',
    help="Run this process as a daemon in the background.")
parser.add_argument('-k', '--kill-daemon', required=False, dest='kill_daemon', default=False, action='store_true',
    help="Kill the currently processing process.")
parser.add_argument('--force-alerts', required=False, dest='force_alerts', default=False, action='store_true',
    help="Force all analysis to always generate an alert.")
parser.add_argument('--relative-dir', required=False, dest='relative_dir', default=None,
    help="Assume all storage paths are relative to the given directory.  Defaults to current work directory.")
parser.add_argument('-p', '--provide-decryption-password', required=False, action='store_true', dest='provide_decryption_password', default=False,
    help="Prompt for the decryption password. Read README.CRYPTO for details.")
parser.add_argument('-P', '--set-decryption-password', dest='set_decryption_password', default=None,
    help="Provide the decryption password on the command line. Not secure at all. Don't do it.")
parser.add_argument('--trace', required=False, action='store_true', dest='trace', default=False,
    help="Enable execution tracing (debugging option).")
parser.add_argument('-D', required=False, action='store_true', dest='debug_on_error', default=False,
    help="Break into pdb if an unhanled exception is thrown or an assertion fails.")
parser.add_argument('--skip-initialize-automation-user', action='store_true', dest='skip_initialize_automation_user', default=True,
    help="Skip the step of initializing the automation user.")

subparsers = parser.add_subparsers(dest='cmd')

# utility functions
def disable_proxy():
    for proxy_setting in [ 'http_proxy', 'https_proxy', 'ftp_proxy' ]:
        if proxy_setting in os.environ:
            logging.debug("removing proxy setting {}".format(proxy_setting))
            del os.environ[proxy_setting]

def recurse_analysis(analysis, level=0, current_tree=[]):
    """Used to generate a textual display of the analysis results."""
    if not analysis:
        return

    if analysis in current_tree:
        return

    current_tree.append(analysis)

    if level > 0 and len(analysis.observables) == 0 and len(analysis.tags) == 0 and analysis.summary is None:
        return

    display = '{}{}{}'.format('\t' * level, 
                              '<' + '!' * len(analysis.detections) + '> ' if analysis.detections else '', 
                              analysis.summary if analysis.summary is not None else str(analysis))
    if analysis.tags:
        display += ' [ {} ] '.format(', '.join([x.name for x in analysis.tags]))
    
    print(display)

    for summary_detail in analysis.summary_details:
        print('{}{}'.format('\t' * level, summary_detail.content))

    for observable in analysis.observables:
        display = '{} * {}{}:{}'.format('\t' * level, 
                                        '<' + '!' * len(observable.detections) + '> ' if observable.detections else '', 
                                         observable.type, 
                                         observable.value)
        if observable.time is not None:
            display += ' @ {0}'.format(observable.time)
        if observable.directives:
            display += ' {{ {} }} '.format(', '.join([x for x in observable.directives]))
        if observable.tags:
            display += ' [ {} ] '.format(', '.join([x.name for x in observable.tags]))
        if observable.volatile:
            display += ' <VOLATILE> '
        #if observable.pivot_links:
            #for pivot_link in observable.pivot_links:
                #display += f' ðŸ”— {pivot_link}'
        print(display)

        for observable_analysis in observable.all_analysis:
            recurse_analysis(observable_analysis, level + 1, current_tree)

def display_analysis(root):
    recurse_analysis(root)

    tags = set(root.all_tags)
    if tags:
        print("{} TAGS".format(len(tags)))
        for tag in tags:
            print('* {}'.format(tag))

    detections = root.all_detection_points
    if detections:
        print("{} DETECTIONS FOUND (marked with <!> above)".format(len(detections)))
        for detection in detections:
            print('* {}'.format(detection))

# ============================================================================
# hunting utilities
#

def execute_hunt(args):
    import ace_api
    import pytz
    from saq.constants import EVENT_TIME_FORMAT_TZ
    from saq.collectors.hunter import HunterCollector
    from saq.collectors.query_hunter import QueryHunt
    collector = HunterCollector()
    collector.load_hunt_managers()
    
    hunt_type, hunt_name = args.hunt.split(':', 1)
    if hunt_type not in collector.hunt_managers:
        logging.error(f"invalid hunt type {hunt_type}")
        sys.exit(1)

    collector.hunt_managers[hunt_type].load_hunts_from_config()
    hunt = collector.hunt_managers[hunt_type].get_hunt(lambda hunt: hunt_name in hunt.ini_path)
    if hunt is None:
        logging.error(f"unknown hunt {hunt_name} for type {hunt_type}")
        sys.exit(1)

    # set the Hunt to manual so we don't record the execution timestamps
    hunt.manual_hunt = True
    exec_kwargs = {}

    if isinstance(hunt, QueryHunt):
        start_time = datetime.datetime.strptime(args.start_time, '%m/%d/%Y:%H:%M:%S')
        end_time = datetime.datetime.strptime(args.end_time, '%m/%d/%Y:%H:%M:%S')
        if args.timezone is not None:
            start_time = pytz.timezone(args.timezone).localize(start_time)
            end_time = pytz.timezone(args.timezone).localize(end_time)
        else:
            start_time = pytz.utc.localize(start_time)
            end_time = pytz.utc.localize(end_time)

        exec_kwargs['start_time'] = start_time
        exec_kwargs['end_time'] = end_time

        hunt.query_result_file = args.query_result_file

    if args.json_dir is not None:
        os.makedirs(args.json_dir, exist_ok=True)

    json_dir_index = 0
    for submission in hunt.execute(**exec_kwargs):
        print(submission.description)
        if args.details:
            for o in submission.observables:
                output = f"\t(*) {o['type']} - {o['value']}"
                if 'time' in o:
                    output += " - {}".format(o['time'].strftime(EVENT_TIME_FORMAT_TZ))
                if 'tags' in o:
                    output += " tags [{}]".format(','.join(o['tags']))
                if 'directives' in o:
                    output += " direc [{}]".format(','.join(o['directives']))
                if 'pivot_links' in o:
                    for pv in o["pivot_links"]:
                        output += f" pivot ({pv['url']})[{pv['text']}]"
                # TODO the other stuff
                print(output)

            for t in submission.tags:
                print(f"\t(+) {t}")

            if submission.extensions:
                for url in submission.extensions.get("pivot_links", []):
                    print(f"\tðŸ”— {url}")
            
        if args.events:
            print("BEGIN EVENTS")
            for event in submission.details:
                print(event)
            print("END EVENTS")

        if args.json_dir is not None:
            buffer = []
            for event in submission.details:
                buffer.append(event)

            target_json_file = os.path.join(args.json_dir, '{}.json'.format(str(json_dir_index)))
            with open(target_json_file, 'w') as fp:
                json.dump(buffer, fp)

            with open(os.path.join(args.json_dir, 'manifest'), 'a') as fp:
                fp.write(f'{json_dir_index} = {submission.description}\n')

            json_dir_index += 1

        if args.submit_alerts is not None:
            result = ace_api.submit(
                submission.description,
                remote_host=args.submit_alerts,
                ssl_verification=get_config()['SSL']['ca_chain_path'],
                analysis_mode=submission.analysis_mode,
                tool=submission.tool,
                tool_instance=submission.tool_instance,
                type=submission.type,
                event_time=submission.event_time,
                details=submission.details,
                observables=submission.observables,
                tags=submission.tags,
                queue=submission.queue,
                instructions=submission.instructions,
                extensions=submission.extensions,
                files=[])

hunt_parser = subparsers.add_parser('hunt')
hunt_sp = hunt_parser.add_subparsers(dest='hunt_cmd')

execute_hunt_parser = hunt_sp.add_parser('execute',
    help="Execute a hunt with the given parameters.")
execute_hunt_parser.add_argument('hunt',
    help="The name of the hunt to execute in the format type:name where type is the hunt type.")
execute_hunt_parser.add_argument('-s', '--start-time', required=False, default=None,
    help="Optional start time. Time spec absolute format is MM/DD/YYYY:HH:MM:SS")
execute_hunt_parser.add_argument('-e', '--end-time', required=False, default=None,
    help="Optional end time. Time spec absolute format is MM/DD/YYYY:HH:MM:SS")
execute_hunt_parser.add_argument('-z', '--timezone', required=False, default=None,
    help="Optional time zone for start time and end time. Defaults to local time zone.")
execute_hunt_parser.add_argument('-v', '--events', required=False, default=False, action='store_true',
    help="Output the events instead of the submissions.")
execute_hunt_parser.add_argument('--json-dir', required=False, default=None,
    help="Store the events as JSON files in the given directory, one per submission created.")
execute_hunt_parser.add_argument('-d', '--details', required=False, default=False, action='store_true',
    help="Include the details of the submissions in the output.")
execute_hunt_parser.add_argument('--submit-alerts', required=False, default=None, 
    help="Submit as alerts to the given host[:port]")
execute_hunt_parser.add_argument('--query-result-file', required=False, default=None,
    help="Valid only for query hunts. Save the raw query results to the given file.")
execute_hunt_parser.set_defaults(func=execute_hunt)

def verify_hunt(args):
    from saq.collectors.hunter import HunterCollector
    collector = HunterCollector()
    collector.load_hunt_managers()
    failed = False
    for hunt_type, manager in collector.hunt_managers.items():
        manager.load_hunts_from_config()
        if manager.failed_ini_files:
            sys.stderr.write(f"ERROR: unable to load {len(manager.failed_ini_files)} {hunt_type} hunts\n")
            failed = True

    if failed:
        sys.exit(1)

    print("hunt syntax verified")
    sys.exit(0)

verify_hunt_parser = hunt_sp.add_parser('verify',
    help="Verifies that all configured hunts are able to load.")
verify_hunt_parser.set_defaults(func=verify_hunt)

def list_hunts(args):
    from saq.collectors.hunter import HunterCollector
    collector = HunterCollector()
    collector.load_hunt_managers()
    for hunt_type, manager in sorted(collector.hunt_managers.items()):
        manager.load_hunts_from_config()
        for hunt in sorted(sorted(manager.hunts, key=operator.attrgetter('name')), 
                key=operator.attrgetter('enabled'), reverse=True):
            ini_file = os.path.splitext(os.path.basename(hunt.ini_path))[0]
            status = "E" if hunt.enabled else "D"
            print(f"{status} {hunt_type}:{ini_file} - {hunt.name}")

    sys.exit(0)

list_hunts_parser = hunt_sp.add_parser('list',
    help="""List the available hunts.
    The format of the output is
    E|D type:name - description
    E: enabled
    D: disabled""")
list_hunts_parser.set_defaults(func=list_hunts)

def list_hunt_types(args):
    from saq.collectors.hunter import HunterCollector
    collector = HunterCollector()
    collector.load_hunt_managers()
    for hunt_type in collector.hunt_managers.keys():
        print(hunt_type)

    sys.exit(0)

list_hunt_types_parser = hunt_sp.add_parser('list-types',
    help="List the available hunting types.")
list_hunt_types_parser.set_defaults(func=list_hunt_types)

def list_saved_searches(args):
    from saq.splunk_ss import load_saved_searches
    for saved_search in load_saved_searches(args.section, args.user, args.app):
        print(saved_search.name)

    sys.exit(0)

list_saved_searches_parser = hunt_sp.add_parser('list-saved-searches',
    help="List all managed saved searches.")
list_saved_searches_parser.add_argument("-s", "--section", default="splunk_cloud",
    help="The splunk configuration section in the INI files to use.")
list_saved_searches_parser.add_argument("-u", "--user", default="nobody",
    help="The splunk services namespace user.")
list_saved_searches_parser.add_argument("-a", "--app", default="ftb_search_infosec",
    help="The splunk services namespace app.")
list_saved_searches_parser.set_defaults(func=list_saved_searches)

def publish_saved_searches(args):
    from saq.splunk_ss import load_ini_files, publish_saved_search
    for saved_search in load_ini_files(args.dir):
        publish_saved_search(saved_search)

    sys.exit(0)

publish_saved_searches_parser = hunt_sp.add_parser('publish-saved-searches',
    help="Publish all saved searches specified in the given directory.")
publish_saved_searches_parser.add_argument("-d", "--dir",
    help="The directory to load the saved searches from.")
publish_saved_searches_parser.set_defaults(func=publish_saved_searches)

def delete_saved_search(args):
    from saq.splunk_ss import delete_saved_search, load_from_ini, SavedSearch
    search = None
    if args.file_path:
        search = load_from_ini(args.file_path)
    else:
        search = SavedSearch(
            name=args.name,
            type=args.section,
            ns_user=args.user,
            ns_app=args.app,
        )

    delete_saved_search(search)
    sys.exit(0)

delete_saved_search_parser = hunt_sp.add_parser('delete-saved-search',
    help="""Deletes a saved search. An existing ini file can be specified, or individual options.
    NOTE if you specify --file you do NOT need to specify the other arguments.
    """)
delete_saved_search_parser.add_argument("-f", "--file-path",
    help="The .savedsearch INI file specifying the saved search to delete.")
delete_saved_search_parser.add_argument("-s", "--section", default="splunk_cloud",
    help="The splunk configuration section in the INI files to use.")
delete_saved_search_parser.add_argument("-u", "--user", default="nobody",
    help="The splunk services namespace user.")
delete_saved_search_parser.add_argument("-a", "--app", default="ftb_search_infosec",
    help="The splunk services namespace app.")
delete_saved_search_parser.add_argument("-n", "--name", default="ftb_search_infosec",
    help="The name of the saved search to delete.")
delete_saved_search_parser.set_defaults(func=delete_saved_search)

def sync_saved_searches(args):
    from saq.splunk_ss import sync_saved_searches
    sync_saved_searches(args.dir, config=args.section, ns_user=args.user, ns_app=args.app)
    sys.exit(0)

sync_saved_searches_parser = hunt_sp.add_parser('sync-saved-searches',
    help="Sync all saved searches specified in the given directory.")
sync_saved_searches_parser.add_argument("-d", "--dir",
    help="The directory to load the saved searches from.")
sync_saved_searches_parser.add_argument("-s", "--section", default="splunk_cloud",
    help="The splunk configuration section in the INI files to use.")
sync_saved_searches_parser.add_argument("-u", "--user", default="nobody",
    help="The splunk services namespace user.")
sync_saved_searches_parser.add_argument("-a", "--app", default="ftb_search_infosec",
    help="The splunk services namespace app.")
sync_saved_searches_parser.set_defaults(func=sync_saved_searches)


# ============================================================================
# persistence data
#

persistence_parser = subparsers.add_parser('persistence', aliases=['per'])
persistence_sp = persistence_parser.add_subparsers(dest='persistence_cmd')

def list_persistence(args):
    source_query = get_db().query(PersistenceSource)
    if args.source:
        source_query = source_query.filter(PersistenceSource.name.like('%{}%'.format(args.source)))

    for source in source_query.order_by(PersistenceSource.name):
        if args.keys or args.name:
            query = get_db().query(Persistence).filter(Persistence.source_id == source.id)
            if args.name:
                 query = query.filter(Persistence.uuid.like(f'%{args.name}%'))
            if not args.temporal:
                query = query.filter(Persistence.permanent == 1)

            for persistence in query.order_by(Persistence.uuid):
                print(f"{source.name} {persistence.uuid}")
        else:
            print(source.name)

    sys.exit(0)

list_persistence_parser = persistence_sp.add_parser('list',
    help="List all persistence sources.")
list_persistence_parser.add_argument('-k', '--keys', action='store_true', default=False,
    help="Also display permanent keys. Use --search to narrow down results.")
list_persistence_parser.add_argument('-s', '--source',
    help="Search for keys from the given source.")
list_persistence_parser.add_argument('-n', '--name',
    help="Search for key names matching the given pattern.")
list_persistence_parser.add_argument('-t', '--temporal', action='store_true', default=False,
    help="Also display temporal (non-permanent) keys.")
list_persistence_parser.set_defaults(func=list_persistence)

def clear_persistence(args):
    import dateparser
    from saq.database import Persistence, PersistenceSource, and_

    source = get_db().query(PersistenceSource).filter(PersistenceSource.name == args.source).first()
    if source is None:
        logging.error(f"unknown persistence source {args.source}")
        sys.exit(1)

    if args.all:
        result = get_db().execute(Persistence.__table__.delete().where(Persistence.source_id == source.id))
        logging.info(f"persistence group {args.source} cleared {result.rowcount} items")

        if not args.dry_run:
            get_db().commit()

        sys.exit(0)

    elif args.older_than:
        target_date = dateparser.parse(args.older_than)
        result = get_db().execute(Persistence.__table__.delete().where(and_(Persistence.source_id == source.id,
                                                                          Persistence.permanent == 0,
                                                                          Persistence.last_update < target_date)))

        logging.info(f"persistence group {args.source} cleared {result.rowcount} items")

        if not args.dry_run:
            get_db().commit()

        sys.exit(0)
        
    for key in args.keys:
        result = get_db().execute(Persistence.__table__.delete().where(and_(Persistence.source_id == source.id,
                                                                          Persistence.uuid == key)))

        logging.info(f"persistence group {args.source} key {key} cleared {result.rowcount} items")

    if not args.dry_run:
        get_db().commit()

    sys.exit(0)

clear_persistence_parser = persistence_sp.add_parser('clear',
    help="Clears persistence data for the given source.")
clear_persistence_parser.add_argument('source',
    help="The name of the source to clear.")
clear_persistence_parser.add_argument('keys', nargs="*",
    help="One or more keys to clear.")
clear_persistence_parser.add_argument('--all', action='store_true', default=False,
    help="Clear all persistence data for this source.")
clear_persistence_parser.add_argument('--older-than',
    help="Clear all non-permanent persistence data that is older than a given time.")
clear_persistence_parser.add_argument('--dry-run', action='store_true', default=False,
    help="Do no commit the changes, only report how many would be cleared out.")
    
clear_persistence_parser.set_defaults(func=clear_persistence)

# ============================================================================
# configuration settings
#

def enable_integration(args):
    import saq.integration
    saq.integration.enable_integration(args.integration)
    sys.exit(0)

def disable_integration(args):
    import saq.integration
    saq.integration.disable_integration(args.integration)
    sys.exit(0)

def list_integrations(args):
    import saq.integration
    saq.integration.list_integrations()
    sys.exit(0)

integration_parser = subparsers.add_parser('integration')
integration_sp = integration_parser.add_subparsers(dest='integration_cmd')

enable_integration_parser = integration_sp.add_parser('enable',
    help="Enables the given integration.")
enable_integration_parser.add_argument('integration',
    help="The integration to enable. Use ace integration list to get the list of available integrations.")
enable_integration_parser.set_defaults(func=enable_integration)

list_integration_parser = integration_sp.add_parser('list',
    help="List the available integrations and show enabled/disabled status.")
list_integration_parser.set_defaults(func=list_integrations)

disable_integration_parser = integration_sp.add_parser('disable',
    help="Disables the given integration.")
disable_integration_parser.add_argument('integration',
    help="The integration to disable. Use ace integration list to get the list of available integrations.")
disable_integration_parser.set_defaults(func=disable_integration)

# ============================================================================
# service control/start/stop
#

service_parser = subparsers.add_parser('service',
    help="Service management commands. See ace service --help for details.")
service_sp = service_parser.add_subparsers(dest='service_cmd')

def start_service(args):
    from saq.service import load_service_by_name
    service = load_service_by_name(args.service)

    signal.signal(signal.SIGTERM, lambda signum, frame: service.stop())

    try:
        service.start()
        service.wait()
    except KeyboardInterrupt:
        logging.info(f"caught keyboard interrupt, stopping service {args.service}")
        service.stop()
        service.wait()
        sys.exit(0)
    except Exception as e:
        logging.error(f"error starting service {args.service}: {e}")
        report_exception()
        sys.exit(1)

    logging.info(f"service {args.service} exited")
    sys.exit(0)

start_service_parser = service_sp.add_parser('start', help="Start an ACE service.")
start_service_parser.add_argument('service', help="The name of the service to start. This is referenced in the configuration as [service_<name>]")
start_service_parser.set_defaults(func=start_service)

# ============================================================================
# export observables
#

# XXX not sure we need this anymore

def export_observables(args):
    """Exports observables, mappings and some alert context into a sqlite database."""
    from saq.database import get_db_connection

    if os.path.exists(args.output_file):
        try:
            os.remove(args.output_file)
        except Exception as e:
            logging.error("unable to delete {}: {}".format(args.output_file, e))
            sys.exit(1)

    output_db = sqlite3.connect(args.output_file)
    output_c = output_db.cursor()
    output_c.execute("""CREATE TABLE observables ( type text NOT NULL, value text NOT NULL)""")
    output_c.execute("""CREATE UNIQUE INDEX i_type_value ON observables( type, value )""")
    output_c.execute("""CREATE INDEX i_value ON observables( value )""")

    i = 0
    skip = 0
    with get_db_connection() as input_db:
        input_c = input_db.cursor()
        input_c.execute("""SELECT type, value FROM observables""")
        for _type, _value in input_c:
            try:
                _value = _value.decode('utf-8')
            except UnicodeDecodeError:
                skip += 1
                continue

            output_c.execute("""INSERT INTO observables ( type, value ) VALUES ( ?, ? )""", (_type, _value))
            i += 1

    output_db.commit()
    output_db.close()

    logging.info("exported {} observables (skipped {})".format(i, skip))
    sys.exit(0)

export_observables_parser = subparsers.add_parser('export-observables',
    help="Exports observables into a sqlite database.")
export_observables_parser.add_argument('output_file', 
    help="Path to the sqlite database to create. Existing files are overwritten.")
export_observables_parser.set_defaults(func=export_observables)
    

# ============================================================================
# command line correlation
#

def correlate(args):
    # initialize command line engine
    from saq.analysis import RootAnalysis
    from saq.constants import F_FILE, ANALYSIS_MODE_CLI, ANALYSIS_MODE_CORRELATION
    from saq.engine.core import Engine
    from saq.util import parse_event_time


    if len(args.targets) % 2 != 0:
        logging.error("odd number of arguments (you need pairs of type and value)")
        sys.exit(1)

    targets = args.targets
    
    # did we specify targets from stdin?
    if args.from_stdin:
        for o_value in sys.stdin:
            # the type of observables coming in on stdin is also specified on the command line
            targets.append(args.stdin_type)
            targets.append(o_value.strip())

    reference_time = None
    if args.reference_time is not None:
        reference_time = parse_event_time(args.reference_time)

    if os.path.exists(args.storage_dir) and not args.load:
        # if the output directory is the default directory then just delete it
        # this has been what I've wanted to happen 100% of the time
        if args.storage_dir == 'ace.out':
            try:
                logging.warning("deleting existing output directory ace.out")
                shutil.rmtree('ace.out')
            except Exception as e:
                logging.error("unable to delete existing output directory ace.out: {}".format(e))
                sys.exit(1)
        else:
            logging.error("output directory {} already exists".format(args.storage_dir))
            sys.exit(1)

    # the list of root analyses to analyze
    roots: list[RootAnalysis] = []

    if args.uuid is not None:
        root_uuid = args.uuid
    else:
        root_uuid = str(uuid.uuid4())

    root = RootAnalysis(uuid=root_uuid, storage_dir=args.storage_dir)

    if os.path.exists(args.storage_dir):
        logging.warning("storage directory {} already exists".format(args.storage_dir))
    else:
        # create the output directory
        try:
            root.initialize_storage()
        except Exception as e:
            logging.error("unable to create output directory {}: {}".format(args.storage_dir, e))
            sys.exit(1)

    if args.load:
        root.load()

        # we override whatever previous analysis mode it had
        root.analysis_mode = ANALYSIS_MODE_CLI if args.analysis_mode is None else args.analysis_mode

    else:
        # set all of the properties individually
        # XXX only require company_id in RootAnalysis
        if args.company_name:
            root.company_name = args.company_name

        root.tool = 'ACE - Command Line Analysis'
        root.tool_instance = socket.gethostname()
        root.alert_type = args.alert_type
        root.analysis_mode = ANALYSIS_MODE_CLI if args.analysis_mode is None else args.analysis_mode
        if f'analysis_mode_{root.analysis_mode}' not in get_config():
            logging.error(f"invalid analysis mode {root.analysis_mode}")
            sys.exit(1)

        # disable cleanup in whatever mode we use
        get_config()[f'analysis_mode_{root.analysis_mode}']['cleanup'] = 'no'

        root.description = args.description if args.description else 'Command Line Correlation'
        root.instructions = args.instructions
        root.event_time = datetime.datetime.now() if reference_time is None else reference_time
        if args.load_details:
            with open(args.load_details, 'r') as fp:
                root.details = json.load(fp)
        else:
            root.details = { 
                'local_user': pwd.getpwuid(os.getuid())[0],
                'local_user_uid': os.getuid(),
                'comment': args.comment
            }

    # create the list of observables to add to the alert for analysis
    index = 0
    while index < len(args.targets):
        o_type = args.targets[index]
        o_value = args.targets[index + 1]

        #if o_type not in VALID_OBSERVABLE_TYPES:
            #logging.error("invalid observable type {0}".format(o_type))
            #sys.exit(1)

        # the root analysis we're currently working on (defaults to the main alert)
        current_root = root

        # are we creating a separate alert for each observable?
        if args.split:
            subdir = os.path.join(root.storage_dir, current_root.uuid[0:3])
            if not os.path.exists(subdir):
                try:
                    os.mkdir(subdir)
                except Exception as e:
                    logging.error("unable to create directory {}: {}".format(subdir, e))
                    sys.exit(1)

            current_root = RootAnalysis(storage_dir=os.path.join(subdir, current_root.uuid))
            try:
                current_root.initialize_storage()
            except Exception as e:
                logging.error("unable to create directory {}: {}".format(subdir, e))

            # XXX not sure we need this any more
            # we'll make a little symlink if we can to help analysts know which directory is what
            # it's ok if this fails
            try:
                os.symlink(os.path.join(current_root.uuid[0:3], current_root.uuid), os.path.join(root.storage_dir, str(o_value)))
            except Exception as e:
                logging.warning("unable to create symlink: {}".format(e))

            current_root.tool = root.tool
            current_root.tool_instance = root.tool_instance
            current_root.alert_type = root.alert_type
            current_root.analysis_mode = root.analysis_mode
            current_root.description = "{} - {}:{}".format(root.description, o_type, o_value)
            current_root.event_time = root.event_time
            current_root.details = root.details

        # if this is a file then we need to copy it over to the storage directory
        #if o_type == F_FILE:
            #dest_path = os.path.join(current_root.storage_dir, os.path.basename(o_value))
            #try:
                #logging.debug("copying {} to {}".format(o_value, dest_path))
                #shutil.copy(o_value, dest_path)
            #except Exception as e:
                #logging.error("unable to copy {} to {} for analysis: {}".format(o_value, dest_path, e))
                #sys.exit(1)

            #o_value = os.path.basename(o_value)

        if observable is not None:
            if o_type == F_FILE:
                observable = current_root.add_file_observable(o_value, volatile=args.volatile)
            else:
                observable = current_root.add_observable(o_type, o_value, reference_time, volatile=args.volatile)

            for directive in args.directives:
                observable.add_directive(directive)

            for tag in args.tags:
                observable.add_tag(tag)

        index += 2

        if args.split:
            current_root.save()
            roots.append(current_root)

    # if we are not splitting up the alerts then we just have one alert to look at
    if not args.split:
        root.save()
        roots.append(root)

    engine = Engine(config = EngineConfiguration(engine_type=EngineType.LOCAL, single_threaded_mode=not args.multi_threaded))
    engine.configuration_manager.config.alerting_enabled = False
    worker = engine.initialize_single_threaded_worker()

    for root in roots:
        worker.workload_manager.add_workload(root)

    # allow the user to control what analysis modules run
    if args.disable_all:
        logging.warning("disabling all analysis modules...")
        for section in get_config().sections():
            if not section.startswith('analysis_module_'):
                continue

            get_config()[section]['enabled'] = 'no'

    if args.disabled_modules:
        for section in get_config().sections():
            if not section.startswith('analysis_module_'):
                continue

            for name_pattern in args.disabled_modules:
                if name_pattern in section[len('analysis_module_'):]:
                    logging.warning("disabling {}".format(section))
                    get_config()[section]['enabled'] = 'no'

    # enable by group
    if args.enable_module_group:
        for module_group in args.enable_module_group:
            group_section = 'module_group_{}'.format(module_group)
            if group_section not in get_config():
                logging.error("module group {} does not exist".format(module_group))
                sys.exit(1)

            for module_name in get_config()[group_section].keys():
                logging.info("enabling {} by group {}".format(module_name, module_group))
                engine.configuration_manager.enable_module(module_name, ANALYSIS_MODE_CLI)

    if args.enabled_modules:
        for name_pattern in args.enabled_modules:
            for section in get_config().sections():
                if fnmatch.fnmatch(section[len('analysis_module_'):], name_pattern):
                    logging.info("enabling {}".format(section))
                    engine.configuration_manager.enable_module(section, ANALYSIS_MODE_CLI)

    try:
        engine.start_single_threaded(execution_mode=EngineExecutionMode.UNTIL_COMPLETE)
    except KeyboardInterrupt:
        logging.warning("user interrupted correlation")

    for root in roots:
        # display the results
        root = RootAnalysis(storage_dir=root.storage_dir)
        root.load()
        display_analysis(root)

        if args.alert:
            if root.whitelisted:
                logging.info("{} was whitelisted".format(root))
                continue

            if g_boolean(G_FORCED_ALERTS) or root.has_detections():
                from ace_api import upload
                try:
                    logging.info("uploading {}".format(root))

                    # need to switch the mode to correlation
                    root.analysis_mode = ANALYSIS_MODE_CORRELATION
                    root.save()

                    remote_host = None # if left as None then the api call defaults it to ace_api.default_node
                    if args.remote_host is not None:
                        remote_host = args.remote_host

                    if args.remote_port is not None:
                        remote_host = '{}:{}'.format(remote_host, args.remote_port)

                    result = upload(root.uuid, root.storage_dir, remote_host=remote_host, ssl_verification=args.ssl_ca_path)

                except Exception as e:
                    logging.error("unable to upload {}: {}".format(root, e))

    sys.exit(0)

correlate_parser = subparsers.add_parser('correlate',
    help="Analyze one or more observables or alerts.",
    epilog="Example: ace correlate ipv4 8.8.8.8 -G email -E ip_inspector -E gglsbl_service -D process_analysis_v1 -G common")
correlate_parser.add_argument('--multi-threaded', required=False, dest='multi_threaded', default=False, action='store_true',
    help="Use multiple processes to run the analysis.")
correlate_parser.add_argument('-D', '--disable-module', required=False, dest='disabled_modules', action='append',
    help="Specify a module name (substring match) to disable. This option can be specified multiple times.")
correlate_parser.add_argument('--disable-all', required=False, dest='disable_all', default=False, action='store_true',
    help="Disable all analysis modules (use -E switch to enable specific modules.")
correlate_parser.add_argument('-E', '--enable-module', required=False, dest='enabled_modules', action='append', 
    help="Specify module names (substring match) to enable. This option can be specified multiple times.")
correlate_parser.add_argument('-G', '--enable-module-group', required=False, dest='enable_module_group', action='append',
    help="Module groups to enable by name. Specify for each module group.")
correlate_parser.add_argument('-m', '--analysis-mode', required=False, dest='analysis_mode', 
    help="The analysis mode to use for this analysis. Defaults to cli")
correlate_parser.add_argument('-d', '--storage-dir', required=False, dest='storage_dir', default='ace.out',
    help="Specify an output directory.  Defaults to ace.out.")
correlate_parser.add_argument('-t', '--reference-time', required=False, dest='reference_time', default=None,
    help="Specify a datetime in YYYY-MM-DD HH:MM:SS [+-]0000 format that observables (of a temporal type) should be referenced from.")
correlate_parser.add_argument('--description', required=False, dest='description', default="ACE Manual Correlation",
    help="Supply a description.  This will be displayed as part of the alert if this correlation is later imported as an alert.")
correlate_parser.add_argument('--comment', required=False, dest='comment', default=None,
    help="Optional generic comment to add to the details of the alert.")
correlate_parser.add_argument('--add-directive', required=False, dest='directives', action='append', default=[],
    help="Adds the given directive to all observables specified.  This option can be used multiple times.")
correlate_parser.add_argument('--add-tag', required=False, dest='tags', action='append', default=[],
    help="Adds the given tag to all observables specified.  This option can be used multiple times.")
correlate_parser.add_argument('--volatile', required=False, dest='volatile', action='store_true', default=False,
    help="Adds the observables as volatile obervables.")
correlate_parser.add_argument('--alert-type', required=False, dest='alert_type', default='cli_analysis',
    help="Optionally set the alert type.  Some analysis is only performed for alerts of a certain type.")
correlate_parser.add_argument('--instructions', required=False, dest='instructions', default=None,
    help="""A free form string value that gives the analyst instructions on what
        this alert is about and/or how to analyze the data contained in the
        alert.""")
correlate_parser.add_argument('--company-name', required=False, dest='company_name', default=None,
    help="Optionally assign ownership of this analysis to a company.")
correlate_parser.add_argument('--alert', required=False, dest='alert', action='store_true', default=False,
    help="Insert the correlation as an alert if it contains a detection point.")
correlate_parser.add_argument('--remote-host', required=False, dest='remote_host', default=None,
    help="Specify the remote host of the ACE system (defaults to the engine_ace configuration values).")
correlate_parser.add_argument('--remote-port', required=False, dest='remote_port', default=None, type=int,
    help="Specify the remote port of the ACE system (defaults to the engine_ace configuration values).")
#correlate_parser.add_argument('--ssl-root-cert', required=False, dest='ssl_root_cert', default=None,
    #help="Specify the path to the SSL cert for the ACE system (defaults to the engine_ace configuration values).")
#correlate_parser.add_argument('--ssl-key', required=False, dest='ssl_key', default=None,
    #help="Specify the path to the SSL key for the ACE system (defaults to the engine_ace configuration values).")
correlate_parser.add_argument('--ssl-ca-path', required=False, dest='ssl_ca_path', default=None,
    help="Specify the path to the CA cert for the ACE system (defaults to the engine_ace configuration values).")
#correlate_parser.add_argument('--ssl-hostname', required=False, dest='ssl_hostname', default=None,
    #help="Specify the ssl hostname of the ACE system (defaults to the engine_ace configuration values).")
correlate_parser.add_argument('--split', required=False, dest='split', action='store_true', default=False,
    help="Split the observables up into individual analysis.")
correlate_parser.add_argument('--from-stdin', required=False, dest='from_stdin', action='store_true', default=False,
    help="Read observables from stanard input.  Defaults to treating them as file-type observables.")
correlate_parser.add_argument('--stdin-type', required=False, dest='stdin_type', default='file',
    help="Specify the observable type when reading observables from stdin. Defaults to file.")
#correlate_parser.add_argument('--skip-analysis', required=False, dest='skip_analysis', action='store_true', default=False,
    #help="Skip analyzing the alert. Useful if you just want to send a bunch of stuff to ACE for analysis.")
correlate_parser.add_argument('--load', required=False, dest='load', action='store_true', default=False,
    help="Instead of creating a new analysis, load the existing analysis stored at --storage-dir.")
correlate_parser.add_argument('--load-details', required=False, dest='load_details', default=None,
    help="Load the given JSON file as the details of the alert.")
correlate_parser.add_argument('--uuid', required=False, dest='uuid', default=None,
    help="Use this uuid as the uuid of the temporary root analysis. Defaults to random if not specified.")
correlate_parser.add_argument('targets', nargs="*",
    help="One or more pairs of indicator types and values.")
correlate_parser.set_defaults(func=correlate)

# ============================================================================
# Remediation Utilities
#

remediation_parser = subparsers.add_parser('remediation')
remediation_sp = remediation_parser.add_subparsers(dest='remediation_cmd')

def print_remediation_status(id=None, type=None, status=None, key=None, user_id=None, user=None):
    from saq.database import Remediation, User
    from sqlalchemy import or_

    query = get_db().query(Remediation).join(User)

    if id:
        query = query.filter(Remediation.id == id)

    if type:
        query = query.filter(Remediation.type == type)

    if status:
        query = query.filter(Remediation.status == status)

    if key:
        query = query.filter(Remediation.key.ilike(f'%{key}%'))

    if user_id:
        query = query.filter(Remediation.user_id == user_id)

    if user:
        query = query.filter(or_(User.username.ilike(f'%{user}%'), 
                                 User.display_name.ilike(f'%{user}%')))

    for r in query.order_by(Remediation.insert_date.desc()):
        print(f"insert_date: {r.insert_date} id:{r.id} type:{r.type} action:{r.action} "
              f'user_id: {r.user_id} key: "{r.key}" result: "{r.result}" comment: "{r.comment}" '
              f'successful: {r.successful} company_id: {r.company_id} lock: {r.lock} lock_time: {r.lock_time} '
              f'status: {r.status}')

def remediation_status(args):
    print_remediation_status(status=args.status,
                             type=args.type,
                             key=args.key,
                             user=args.user,
                             user_id=args.user_id)
    sys.exit(0)

remediation_status_parser = remediation_sp.add_parser('status',
    help="Shows the status of remediation requests.")
remediation_status_parser.add_argument('-s', '--status',
    help="Return results matching the given status.")
remediation_status_parser.add_argument('-t', '--type',
    help="Return results matching the given remediation type.")
remediation_status_parser.add_argument('-k', '--key', 
    help="Return results matching the given remediation target (key).")
remediation_status_parser.add_argument('-u', '--user', 
    help="Return results matching the given username or display name.")
remediation_status_parser.add_argument('--user-id', type=int,
    help="Return results matching the given user_id.")
remediation_status_parser.set_defaults(func=remediation_status)

# most of the arguments are shared between the remove and restore action
def _add_remediation_common_arguments(_parser):
    _parser.add_argument('-b', '--background', action='store_true', default=False,
        help="Executes the action in the background.")
    _parser.add_argument('-c', '--comment', default="command line manual request",
        help="Adds the given comment to the remediation as useful note.")
    _parser.add_argument('--from-file', required=False, default=None,
        help="Read the list of targets, one per line, from the given file.")
    _parser.add_argument('--from-stdin', required=False, default=False, action='store_true',
        help="Read the list of targets, one per line, from the standard input.")

def _add_remediation_type_value_arguments(_parser):
    _parser.add_argument('type',
        help="The type of thing you want to target.")
    _parser.add_argument('target', nargs='*',
        help="The thing or things you want to target.")

# ============================================================================
# start GUI (non-apache version)
#

def start_gui(args):
    from app import create_app
    from werkzeug.serving import run_simple
    from werkzeug.middleware.dispatcher import DispatcherMiddleware

    app = create_app()
    app.jinja_env.auto_reload = True
    app.config['TEMPLATES_AUTO_RELOAD'] = True
    app.config['DEBUG'] = True
    app.config['APPLICATION_ROOT'] = '/ace'
    app.config['SESSION_COOKIE_PATH'] = '/'
    app.wsgi_app = DispatcherMiddleware(app.wsgi_app, {
        app.config['APPLICATION_ROOT']: app,
    })

    # add the "do" template command
    app.jinja_env.add_extension('jinja2.ext.do')

    if args.print_uri_paths:
        for rule in app.url_map.iter_rules():
            print(rule)
        sys.exit(0)

    # This ensures that the development environment returns unique objects every time something in the GUI
    # uses get_db().query() to fetch something from the database. Without this, it would serve stale data to
    # the application. For instance, this is the cause in the development containers of sometimes the event
    # pages saying "Alerts are still analyzing" and not showing FA Queue results when in fact the alerts
    # were finished analyzing.
    #get_db() = get_db().session

    logging.info("ssl_cert = %s exists %s", get_config().get("gui", "ssl_cert"), os.path.exists(get_config().get("gui", "ssl_cert")))
    logging.info("ssl_key = %s exists %s", get_config().get("gui", "ssl_key"), os.path.exists(get_config().get("gui", "ssl_key")))

    listen_address = args.address or get_config_value(CONFIG_GUI, CONFIG_GUI_LISTEN_ADDRESS)
    listen_port = args.port or get_config_value_as_int(CONFIG_GUI, CONFIG_GUI_LISTEN_PORT)
    #run_simple(get_config().get('gui', 'listen_address'), get_config().getint('gui', 'listen_port'), app,
    run_simple(listen_address, listen_port, app,
               #ssl_context=(get_config().get('gui', 'ssl_cert'), get_config().get('gui', 'ssl_key')),
               ssl_context="adhoc",
               use_reloader=True)

# start-gui
start_gui_parser = subparsers.add_parser('start-gui',
    help="Start the SAQ GUI.")
start_gui_parser.add_argument('args', nargs=argparse.REMAINDER,
    help="Parameters to pass to the GUI command shell.")
start_gui_parser.add_argument("--address", default=None, help="Address to bind to. Defaults to configuration setting.")
start_gui_parser.add_argument("--port", default=None, type=int, help="Port to bind to. Defaults to configuration setting.")
start_gui_parser.add_argument('--print-uri-paths', default=False, action='store_true',
    help="Print all of the availble URL paths and exit, without starting the GUI.")
start_gui_parser.set_defaults(func=start_gui)

# ============================================================================
# start API (non-apache version)
#

def start_api(args):
    from aceapi import create_app

    app = create_app(testing=True)
    from werkzeug.serving import run_simple
    from werkzeug.middleware.dispatcher import DispatcherMiddleware
    app.config['DEBUG'] = True
    app.config['APPLICATION_ROOT'] = '/api'
    app.wsgi_app = DispatcherMiddleware(app.wsgi_app, {
        app.config['APPLICATION_ROOT']: app,
    })

    if args.print_uri_paths:
        for rule in app.url_map.iter_rules():
            print(rule)
        sys.exit(0)

    run_simple(get_config().get('api', 'listen_address'), get_config().getint('api', 'listen_port'), app,
               ssl_context=(get_config().get('api', 'ssl_cert'), get_config().get('api', 'ssl_key')),
               use_reloader=False)

# start-gui
start_api_parser = subparsers.add_parser('start-api',
    help="Start the ACE API server in DEBUG mode.")
start_api_parser.add_argument('args', nargs=argparse.REMAINDER,
    help="Parameters to pass to the API command shell.")
start_api_parser.add_argument('--print-uri-paths', default=False, action='store_true',
    help="Print all of the availble URL paths and exit, without starting the API.")
start_api_parser.set_defaults(func=start_api)

# ============================================================================
# user management
#

def add_user(args):
    from app.models import User
    from saq.database import get_db_connection
    from saq.database.util.user_management import add_user
    from getpass import getpass

    u = User()
    u.username = args.username
    u.email = args.email
    u.display_name = args.display_name
    u.queue = args.queue

    try:
        import pytz
        u.timezone = 'Etc/UTC'
        if args.timezone:
            u.timezone = args.timezone

        pytz.timezone(u.timezone)

    except Exception:
        print(f"ERROR: invalid timezone {u.timezone}")
        sys.exit(1)

    if args.password:
        password = args.password
    else:
        password = getpass("Enter password for {}: ".format(u.username))
        confirm = getpass("Confirm password for {}: ".format(u.username))
        if password != confirm:
            logging.error("passwords do not match")
            sys.exit(1)
    

    u.password = password

    add_user(u.username, u.email, password, u.timezone, u.display_name, u.queue)

    #with get_db_connection() as db:
        #c = db.cursor()
        #c.execute("""INSERT INTO users ( username, email, password_hash, timezone, display_name, queue ) VALUES ( %s, %s, %s, %s, %s, %s )""", (
            #u.username, u.email, u.password_hash, u.timezone, u.display_name, u.queue ))
        #db.commit()

    logging.info("added user {}".format(u.username))

user_parser = subparsers.add_parser('user',
    help="User management commands.")
user_sp = user_parser.add_subparsers(dest='user_cmd')

add_user_parser = user_sp.add_parser('add',
    help="Add a new user to the system.")
add_user_parser.add_argument('username', help="The username of the new user.")
add_user_parser.add_argument('email', help="The email address of the new user.")
add_user_parser.add_argument('-z', '--timezone', required=False, default=None,
    help="The timezone the user is in. Defaults to UTC.")
add_user_parser.add_argument('-d', '--display-name', required=False, default=None,
    help="The (optional) display name for the user.")
add_user_parser.add_argument('--password', required=False, default=None,
    help="""Provide the password for the user on the command line. 
    Don't do this unless it's a part of automation.""")
add_user_parser.add_argument('-q', '--queue', required=False, default='default', 
    help="Set the default queue the user is assigned to.")
add_user_parser.set_defaults(func=add_user)

# XXX DEPRECATED
add_user_parser = subparsers.add_parser('add-user',
    help="Add a new user to the system.")
add_user_parser.add_argument('username', help="The username of the new user.")
add_user_parser.add_argument('email', help="The email address of the new user.")
add_user_parser.add_argument('-z', '--timezone', required=False, default=None,
    help="The timezone the user is in. Defaults to UTC.")
add_user_parser.add_argument('-d', '--display-name', required=False, default=None,
    help="The (optional) display name for the user.")
add_user_parser.set_defaults(func=add_user)

def modify_user(args):
    from app.models import User
    from saq.database import get_db_connection
    from getpass import getpass
    from aceapi.auth import set_user_api_key, clear_user_api_key

    u = User()
    u.username = args.username

    with get_db_connection() as db:
        c = db.cursor()
        c.execute("""SELECT id FROM users WHERE username = %s""", ( u.username, ))
        row = c.fetchone()
        if row is None:
            logging.error("username {0} does not exist".format(u.username))
            sys.exit(1)

        user_id = row[0]

    if args.email is not None:
        u.email = args.email

    if args.password:
        password = getpass("Enter password for {0}: ".format(u.username))
        confirm = getpass("Confirm password for {0}: ".format(u.username))
        if password != confirm:
            logging.error("passwords do not match")
            sys.exit(1)

        u.password = password
    
    try:
        import pytz
        u.timezone = 'Etc/UTC'
        if args.timezone:
            u.timezone = args.timezone

        pytz.timezone(u.timezone)

    except Exception as e:
        print(f"ERROR: invalid timezone {e}")
        sys.exit(1)

    with get_db_connection() as db:
        c = db.cursor()
        if args.email is not None:
            c.execute("""UPDATE users SET email = %s WHERE id = %s""", ( u.email, user_id ))
            
        if args.password:
            c.execute("""UPDATE users SET password_hash = %s WHERE id = %s""", ( u.password_hash, user_id ))

        if args.timezone:
            c.execute("""UPDATE users SET timezone = %s WHERE id = %s""", ( u.timezone, user_id ))

        if args.default_queue:
            c.execute("""UPDATE users SET queue = %s WHERE id = %s""", ( args.default_queue, user_id ))

        if args.display_name:
            c.execute("""UPDATE users SET display_name = %s WHERE id = %s""", ( args.display_name, user_id ))

        if args.enable:
            c.execute("""UPDATE users SET enabled = True WHERE id = %s""", ( user_id ))

        if args.disable:
            c.execute("""UPDATE users SET enabled = False WHERE id = %s""", ( user_id ))

        db.commit()

    if args.new_api_key:
        api_key = set_user_api_key(user_id)
        print()
        print(f"api key = {api_key}")
        print()

    if args.clear_api_key:
        if clear_user_api_key(user_id):
            logging.info("api key cleared")
        else:
            logging.error("api key not cleared (invalid user or no api key set)")

    logging.info("modified user {0}".format(u.username))

modify_user_parser = user_sp.add_parser('modify',
    help="Modifies an existing user on the system.")
modify_user_parser.add_argument('username', help="The username of the user to modify.")
modify_user_parser.add_argument('-e', '--email', dest='email', default=None, help="The new email address of the user.")
modify_user_parser.add_argument('-p', '--password', action='store_true', dest='password', default=False, help="Prompt for a new password.")
modify_user_parser.add_argument('-z', '--timezone', required=False, default=None, help="The timezone the user is in. Defaults to UTC.")
modify_user_parser.add_argument('-q', '--default_queue', required=False, default=None, help="Change the default queue the user is assigned to.")
modify_user_parser.add_argument('-n', '--display-name', required=False, default=None, help="Change the user's display name.")
modify_user_parser.add_argument('--enable', action='store_true', required=False, default=None, help="Enable the user.")
modify_user_parser.add_argument('--disable', action='store_true', required=False, default=None, help="Disable the user.")
modify_user_parser.add_argument('--new-api-key', action='store_true', required=False, default=False, help="Assign a new API key to the given user. Key is printed to standard out.")
modify_user_parser.add_argument('--clear-api-key', action='store_true', required=False, default=False, help="Disable API access for the given user.")
modify_user_parser.set_defaults(func=modify_user)

# XXX DEPRECATED
modify_user_parser = subparsers.add_parser('modify-user',
    help="Modifies an existing user on the system.")
modify_user_parser.add_argument('username', help="The username of the user to modify.")
modify_user_parser.add_argument('-e', '--email', dest='email', default=None, help="The new email address of the user.")
modify_user_parser.add_argument('-p', '--password', action='store_true', dest='password', default=False, help="Prompt for a new password.")
modify_user_parser.add_argument('-z', '--timezone', required=False, default=None, help="The timezone the user is in. Defaults to UTC.")
modify_user_parser.set_defaults(func=modify_user)

def delete_user(args):
    from saq.database import get_db_connection

    with get_db_connection() as db:
        c = db.cursor()
        c.execute("""DELETE FROM users WHERE username = %s""", ( args.username, ))
        db.commit()

    logging.info("deleted user {0}".format(args.username))

delete_user_parser = subparsers.add_parser('delete-user',
    help="Deletes an existing user from the system.")
delete_user_parser.add_argument('username', help="The username of the user to modify.")
delete_user_parser.set_defaults(func=delete_user)

delete_user_parser = user_sp.add_parser('delete',
    help="Deletes an existing user from the system.")
delete_user_parser.add_argument('username', help="The username of the user to modify.")
delete_user_parser.set_defaults(func=delete_user)

def list_users(args):
    from saq.database import User
    print("{:<6}{:<15}{:<25}{:<40}{:<25}{}".format('Id', 'User', 'Name', 'Queue', 'Enabled', 'TZ'))
    for user in get_db().query(User).order_by(User.username):
        print("{:<6}{:<15}{:<25}{:<40}{:<25}{}".format(
              user.id, 
              user.username, 
              '' if user.display_name is None else user.display_name, 
              user.queue,
              'True' if user.enabled else 'False',
              user.timezone))

    sys.exit(0)

list_users_parser = user_sp.add_parser('list',
    help="List existing users in the system.")
list_users_parser.set_defaults(func=list_users)

def user_exists(args):
    from saq.database import User
    if get_db().query(User).filter(User.username == args.username).one_or_none():
        logging.info("user %s exists", args.username)
        sys.exit(0)

    logging.info("user %s does not exist", args.username)
    sys.exit(1)

exists_user_parser = user_sp.add_parser('exists',
    help="Returns success if the given user exists.")
exists_user_parser.add_argument('username', help="The username of the user to check.")
exists_user_parser.set_defaults(func=user_exists)

# ============================================================================
# test utilities
#

test_parser = subparsers.add_parser('test',
    help="Test commands.")
test_sp = test_parser.add_subparsers(dest='test_cmd')

def test_proxy(args):
    try:
        import requests
        from saq import proxy
        requests.get(args.url, proxies=proxy.proxies(), verify=get_config()['proxy']['verify'] if 'verify' in get_config()['proxy'] else False)
        sys.exit(0)
    except Exception:
        traceback.print_exc()
        sys.exit(1)

test_proxy_parser = test_sp.add_parser('proxy',
    help="Text proxy access")
test_proxy_parser.add_argument('url',
    help="Test the proxy by accessing the given URL. Any content downloaded is discarded.")
test_proxy_parser.set_defaults(func=test_proxy)

def test_database_connections(args):
    from saq.database import get_db_connection

    for key in get_config().keys():
        if key.startswith('database_'):
            if 'hostname' not in get_config()[key] or not get_config()[key]['hostname'] \
            or 'database' not in get_config()[key] or not get_config()[key]['database'] \
            or 'username' not in get_config()[key] or not get_config()[key]['username'] \
            or 'password' not in get_config()[key] or not get_config()[key]['password']:
                print("skipping {}".format(key))
                continue

            db_name = key[len('database_'):]
            print("trying {}...".format(db_name), end='', flush=True)
            try:
                with get_db_connection(db_name) as db:
                    c = db.cursor()
                    c.execute("SELECT 1")
                    row = c.fetchone()
                    print("OK")
            except Exception as e:
                print("FAILED: {}".format(e))
                sys.exit(1)

    sys.exit(0)

test_database_connections_parser = subparsers.add_parser('test-database-connections',
    help="Test the connections to all configured databases.")
test_database_connections_parser.set_defaults(func=test_database_connections)

def test_network_semaphore(args):
    from saq.network_semaphore import NetworkSemaphoreClient
    import time

    client = NetworkSemaphoreClient()
    if client.acquire(args.semaphore_name):
        time.sleep(args.timeout)
        client.release()
    else:
        logging.error("test failed")

network_semaphore_test = subparsers.add_parser('test-network-semaphore',
    help="Test the Network Semaphore Server by requesting a semaphore.")
network_semaphore_test.add_argument('semaphore_name', help="The name of the semaphore to acquire.")
network_semaphore_test.add_argument('-t', '--timeout', required=False, default=60, type=int, dest='timeout',
    help="The number of seconds to wait until the semaphore is released.  Defaults to 60.")
network_semaphore_test.set_defaults(func=test_network_semaphore)

# ============================================================================
# alert management
#

def create_alert(args):
    from saq.analysis import RootAnalysis

    root = RootAnalysis(storage_dir=args.dir)
    root.tool = 'command line'
    root.tool_instance = 'n/a'
    root.alert_type = 'debug'
    root.description = 'Manual Alert'
    root.event_time = datetime.datetime.now()

    root.initialize_storage()

    root.details = { 'description': 'manually created root' }
    root.save()

alert_parser = subparsers.add_parser('alert',
    help="Alert management commands.")
alert_sp = alert_parser.add_subparsers(dest='alert_cmd')

create_alert_parser = subparsers.add_parser('create-alert',
    help="Create a blank alert in the given directory.")
create_alert_parser.add_argument('dir', help="The directory to store the alert in.")
create_alert_parser.set_defaults(func=create_alert)

create_alert_parser = alert_sp.add_parser('create', aliases=['new'],
    help="Create a blank alert in the given directory.")
create_alert_parser.add_argument('dir', help="The directory to store the alert in.")
create_alert_parser.set_defaults(func=create_alert)

def rebuild_index(args):
    """Rebuilds the indexes for the given alerts."""
    from saq.database import Alert, get_db_connection

    storage_dirs = []
    if args.resync_all:
        with get_db_connection() as db:
            c = db.cursor()
            c.execute("""SELECT storage_dir FROM alerts WHERE location = %s""", (g(G_SAQ_NODE),))
            for row in c:
                storage_dirs.append(row[0])
    else:
        storage_dirs = args.dirs

    logging.info("rebuilding indexes for {} alerts".format(len(storage_dirs)))

    for storage_dir in storage_dirs:
        logging.info("rebuilding {}".format(storage_dir))
        alert = get_db().query(Alert).filter(Alert.storage_dir==storage_dir).first()
        if alert is None:
            logging.error(f"missing alert with storage directory {storage_dir}")
            continue

        try:
            if not alert.load():
                logging.error("unable to load {}".format(alert))
                continue

            alert.rebuild_index()

        except Exception as e:
            logging.error("rebuild failure on {}: {} ({})".format(storage_dir, e, type(e)))
            continue

        finally:
            get_db().commit()

    sys.exit(0)

rebuild_index_parser = subparsers.add_parser('rebuild-index',
    help="Rebuilds the indexes for the given alerts.")
rebuild_index_parser.add_argument('--all', default=False, action='store_true', dest='resync_all',
    help="Resyncs all alerts that belong to this node. This can take a long time.")
rebuild_index_parser.add_argument('dirs', nargs='*', default=[], help="One ore more alert directories to resync.")
rebuild_index_parser.set_defaults(func=rebuild_index)

rebuild_index_parser = alert_sp.add_parser('rebuild',
    help="Rebuilds the indexes for the given alerts.")
rebuild_index_parser.add_argument('--all', default=False, action='store_true', dest='resync_all',
    help="Resyncs all alerts that belong to this node. This can take a long time.")
rebuild_index_parser.add_argument('dirs', nargs='*', default=[], help="One ore more alert directories to resync.")
rebuild_index_parser.set_defaults(func=rebuild_index)

def import_alerts(args):
    """Imports one or more alerts from the given directories."""
    import saq
    from saq.constants import ANALYSIS_MODE_CORRELATION
    from saq.database import Alert

    for _dir in args.dirs:
        json_path = os.path.join(_dir, 'data.json')
        if not os.path.exists(json_path):
            logging.error("{} does not exist".format(json_path))
            continue

        # load the alert
        alert = Alert()
        alert.storage_dir = _dir
        if not alert.load():
            logging.error("unable to load {}: try running saq upgrade {}".format(_dir, _dir))
            continue

        # has this already already been imported?
        dest_dir = os.path.join(saq.SAQ_HOME, get_config()['global']['data_dir'], saq.SAQ_NODE, alert.uuid[0:3], alert.uuid)
        if os.path.exists(dest_dir):
            logging.error("ACE storage directory {} already exists".format(dest_dir))
            continue

        try:
            # copy that directory over
            shutil.copytree(_dir, dest_dir)
        except Exception as e:
            logging.error("unable to copy {0} to {1}: {2}".format(
                _dir, dest_dir, str(e)))
            continue

        # remove the old database id if it has one
        alert.id = None
        # and change the storage area
        alert.storage_dir = os.path.relpath(dest_dir, start=saq.SAQ_HOME)

        # are we resetting the alerts?
        if args.reset:
            alert.reset()

        # change a few more things
        alert.location = saq.SAQ_NODE
        alert.company_id = get_config()['global'].getint('company_id')
        alert.company_name = get_config()['global']['company_name']
        alert.analysis_mode = ANALYSIS_MODE_CORRELATION
        alert.disposition = None
        alert.disposition_user_id = None
        alert.disposition_time = None
        alert.owner_id = None
        alert.owner_time = None

        # sync it to the database
        alert.sync()

        # request analysis
        alert.schedule()

        logging.info("imported alert {}".format(alert))

import_alert_parser = subparsers.add_parser('import-alerts',
    help="Import one or more alert directories.")
import_alert_parser.add_argument('-r', '--reset', action='store_true', default=False, dest='reset',
    help="Reset imported alerts.")
import_alert_parser.add_argument('dirs', nargs='+', default=[], help="One ore more alert directories to import.")
import_alert_parser.set_defaults(func=import_alerts)

import_alert_parser = alert_sp.add_parser('import',
    help="Import one or more alert directories.")
import_alert_parser.add_argument('-r', '--reset', action='store_true', default=False, dest='reset',
    help="Reset imported alerts.")
import_alert_parser.add_argument('dirs', nargs='+', default=[], help="One ore more alert directories to import.")
import_alert_parser.set_defaults(func=import_alerts)

def delete_alerts(args):
    """Completely deletes the given alerts from both the storage system and the database."""
    import saq
    from saq.database import Alert, DatabaseSession

    for uuid in args.uuids:
        try:
            # we do them one at a time in case one of them fails
            session = DatabaseSession()
            session.execute(Alert.__table__.delete().where(Alert.uuid == uuid))
            session.commit()
            session.close()
        except Exception as e:
            logging.error("unable to delete alert {0}: {1}".format(uuid, str(e)))

    for uuid in args.uuids:
        storage_dir = os.path.join(saq.SAQ_HOME, get_config()['global']['data_dir'], get_config()['global']['node'], uuid[0:3], uuid)
        if not os.path.exists(storage_dir):
            logging.warning("storage directory {0} does not exist".format(storage_dir))
            continue

        try:
            shutil.rmtree(storage_dir)
        except Exception as e:
            logging.error("unable to delete storage directory {0}: {1}".format(storage_dir, str(e)))

    sys.exit(0)

delete_alert_parser = subparsers.add_parser('delete-alerts',
    help="Delete one or more alerts by UUID.")
delete_alert_parser.add_argument('uuids', nargs='+', default=[], help="One ore more alert UUIDs to delete.")
delete_alert_parser.set_defaults(func=delete_alerts)

delete_alert_parser = alert_sp.add_parser('delete',
    help="Delete one or more alerts by UUID.")
delete_alert_parser.add_argument('uuids', nargs='+', default=[], help="One ore more alert UUIDs to delete.")
delete_alert_parser.set_defaults(func=delete_alerts)

def reset_alerts(args): 
    from saq.analysis import RootAnalysis
    from saq.database import Alert, DatabaseSession

    for storage_dir in args.dirs:
        # get the storage directory of the alert
        if not os.path.exists(storage_dir):
            logging.error("storage directory {0} does not exist".format(storage_dir))
            continue

        session = None

        # try to load it from the database first
        try:
            session = DatabaseSession()
            root = session.query(Alert).filter(Alert.storage_dir==storage_dir).one()
            logging.info("loaded {} from database".format(storage_dir))
        except:
            root = RootAnalysis()
            root.storage_dir = storage_dir
        finally:
            if session:
                session.close()

        try:
            root.load()
        except Exception as e:
            logging.error("unable to load {}: {}".format(root.storage_dir, e))
            continue

        root.reset()
        root.save()

# reset-alerts
reset_alert_parser = subparsers.add_parser('reset-alerts',
    help="Reset the given alerts allowing for re-analysis.")
reset_alert_parser.add_argument('dirs', nargs='+', help="One or more alert directories to reset.")
reset_alert_parser.set_defaults(func=reset_alerts)

reset_alert_parser = alert_sp.add_parser('reset',
    help="Reset the given alerts allowing for re-analysis.")
reset_alert_parser.add_argument('dirs', nargs='+', help="One or more alert directories to reset.")
reset_alert_parser.set_defaults(func=reset_alerts)

def archive_alerts(args):
    from saq.database import Alert

    for storage_dir in args.dirs:
        # get the storage directory of the alert
        if not os.path.exists(storage_dir):
            logging.error("storage directory {} does not exist".format(storage_dir))
            continue

        alert = get_db().session.query(Alert).filter(Alert.storage_dir==storage_dir).first()
        if alert is None:
            logging.warning(f"cannot find alert with storage_dir {storage_dir}")
            continue

        try:
            alert.load()
        except Exception as e:
            logging.error("unable to load {}: {}".format(alert.storage_dir, e))
            continue

        alert.archive()
        alert.save()

# reset-alerts
reset_alert_parser = subparsers.add_parser('archive-alerts',
    help="Archives a given alert by deleting analysis details and external files but keeping observations and tags.")
reset_alert_parser.add_argument('dirs', nargs='+', help="One or more alert directories to archive.")
reset_alert_parser.set_defaults(func=archive_alerts)

reset_alert_parser = alert_sp.add_parser('archive',
    help="Archives a given alert by deleting analysis details and external files but keeping observations and tags.")
reset_alert_parser.add_argument('dirs', nargs='+', help="One or more alert directories to archive.")
reset_alert_parser.set_defaults(func=archive_alerts)

def add_observable(args):

    if args.observable_type not in VALID_OBSERVABLE_TYPES:
        logging.error("invalid observable type {}".format(args.observable_type))
        sys.exit(1)

    # get the alert to modify
    alert = RootAnalysis(storage_dir=args.dir)
    if not alert.lock():
        logging.error("unable to lock alert {}".format(alert))
        sys.exit(1)

    try:
        alert.load()

        if args.observable_type == F_FILE or args.observable_type == F_SUSPECT_FILE:
            try:
                dest_path = os.path.join(alert.storage_dir, os.path.basename(args.observable_value))
                shutil.copy(args.observable_value, dest_path)
                args.observable_value = os.path.relpath(dest_path, start=alert.storage_dir)
            except Exception as e:
                logging.error("unable to copy file into storage directory: {0}".format(str(e)))

        alert.add_observable(args.observable_type, args.observable_value, o_time=args.reference_time)
        alert.save()

    except Exception as e:
        logging.error(str(e))
        traceback.print_exc()
        sys.exit(1)

    finally:
        alert.unlock()

# add-observable
add_observable_parser = subparsers.add_parser('add-observable',
    help="Add an observable to an existing alert and re-analyze.")
add_observable_parser.add_argument('dir', help="The path to the alert to modify.")
add_observable_parser.add_argument('observable_type', help="The type of the observable to add.")
add_observable_parser.add_argument('observable_value', help="The value of the observable.")
add_observable_parser.add_argument('-t', '--reference-time', required=False, dest='reference_time', default=None,
    help="Specify a datetime in YYYY-MM-DD HH:MM:SS format the observable should be referenced from.")
add_observable_parser.set_defaults(func=add_observable)

add_observable_parser = alert_sp.add_parser('add-observable',
    help="Add an observable to an existing alert and re-analyze.")
add_observable_parser.add_argument('dir', help="The path to the alert to modify.")
add_observable_parser.add_argument('observable_type', help="The type of the observable to add.")
add_observable_parser.add_argument('observable_value', help="The value of the observable.")
add_observable_parser.add_argument('-t', '--reference-time', required=False, dest='reference_time', default=None,
    help="Specify a datetime in YYYY-MM-DD HH:MM:SS format the observable should be referenced from.")
add_observable_parser.set_defaults(func=add_observable)

def reload_alerts(args):
    from saq.constants import ANALYSIS_MODE_CORRELATION
    from saq.database import Alert, DatabaseSession

    # generate the list of alerts to reload
    session = DatabaseSession()
    for uuid in args.uuids:
        alert = session.query(Alert).filter(Alert.uuid == uuid).one()
        #alert.request_correlation()
        alert.analysis_mode = ANALYSIS_MODE_CORRELATION
        alert.schedule()

reload_alert_parser = subparsers.add_parser('reload-alerts',
    help="Force analysis (again) on one or more existing alert(s).")
reload_alert_parser.add_argument('uuids', nargs='+',
    help="One or more alert UUIDs to analyze.")
reload_alert_parser.set_defaults(func=reload_alerts)

reload_alert_parser = alert_sp.add_parser('analyze', aliases=['reload'],
    help="Force analysis (again) on one or more existing alert(s).")
reload_alert_parser.add_argument('uuids', nargs='+',
    help="One or more alert UUIDs to analyze.")
reload_alert_parser.set_defaults(func=reload_alerts)

def cleanup_alerts(args):
    """Performs system maintenance.  This is meant to be called from a cron job."""
    from saq.util.maintenance import cleanup_alerts
    cleanup_alerts(fp_days_old=args.fp_days_old, 
                   ignore_days_old=args.ignore_days_old,
                   dry_run=args.dry_run)
    sys.exit(0)

cleanup_alerts_parsers = subparsers.add_parser('cleanup-alerts',
    help="Removes alerts dispositioned as ignore or false positive and older than some amount of time.")
cleanup_alerts_parsers.add_argument('--dry-run', required=False, dest='dry_run', default=False, action='store_true',
    help="Just report how many would be deleted and archived.")
cleanup_alerts_parsers.add_argument('--fp-days-old', type=int, required=False, dest='fp_days_old', default=None, action='store',
    help='Specify how many days old an alert dispositioned as FALSE_POSITIVE should be for it to be archived.')
cleanup_alerts_parsers.add_argument('--ignore-days-old', type=int, required=False, dest='ignore_days_old', default=None, action='store',
    help='Specify how many days old an alert dispositioned as IGNORE should be for it to be deleted.')
#cleanup_alerts_parsers.add_argument('--force-delete', required=False, dest='force_delete', default=None, action='store_true',
    #help='force delete fp alerts instead of archiving them')
cleanup_alerts_parsers.set_defaults(func=cleanup_alerts)

cleanup_alerts_parsers = alert_sp.add_parser('cleanup',
    help="Removes alerts dispositioned as ignore or false positive and older than some amount of time.")
cleanup_alerts_parsers.add_argument('--dry-run', required=False, dest='dry_run', default=False, action='store_true',
    help="Just report how many would be deleted and archived.")
cleanup_alerts_parsers.add_argument('--fp-days-old', type=int, required=False, dest='fp_days_old', default=None, action='store',
    help='Specify how many days old an alert dispositioned as FALSE_POSITIVE should be for it to be archived.')
cleanup_alerts_parsers.add_argument('--ignore-days-old', type=int, required=False, dest='ignore_days_old', default=None, action='store',
    help='Specify how many days old an alert dispositioned as IGNORE should be for it to be deleted.')
#cleanup_alerts_parsers.add_argument('--force-delete', required=False, dest='force_delete', default=None, action='store_true',
    #help='force delete fp alerts instead of archiving them')
cleanup_alerts_parsers.set_defaults(func=cleanup_alerts)

def distribute_alerts(args):
    from saq.configuration import get_config
    from saq.util.maintenance import distribute_old_alerts
    target = args.target
    if not target:
        target = get_config()["global"]["distribution_target"]
    days = args.days
    if not days:
        days = get_config()["global"].getint("distribute_days_old", 90)

    distribute_old_alerts(days, args.dry_run, target, args.max)

distribute_alerts_parsers = subparsers.add_parser('distribute-alerts',
    help="Moves old alerts not associated to events to another node.")
distribute_alerts_parsers.add_argument('--dry-run', required=False, default=False, action='store_true',
    help="Just report how many would be distributed.")
distribute_alerts_parsers.add_argument('--days', type=int,
    help="Specify how many days old an alert should be to be considered for distribution.")
distribute_alerts_parsers.add_argument('--max', type=int, default=0,
    help="Specify the maximum number of alerts to distribute. Defaults to all alerts that match the criteria.")
distribute_alerts_parsers.add_argument('--target',
    help="Optional target. Defaults to the value for distribution_target in the [global] configuration section.")
distribute_alerts_parsers.set_defaults(func=distribute_alerts)

def display_alert(args):
    from saq.analysis import RootAnalysis
    
    alert = RootAnalysis(storage_dir=args.dir)
    try:
        alert.load()
    except Exception as e:
        logging.error("unable to load alert from {}: {}".format(args.dir, str(e)))
        traceback.print_exc()
        sys.exit(1)

    display_analysis(alert)
    sys.exit(0)

display_alert_parser = subparsers.add_parser('display-alert',
    help="Displays the results of the analysis for a given alert.")
display_alert_parser.add_argument('dir', 
    help="The directory of the alert to display")
display_alert_parser.set_defaults(func=display_alert)

display_alert_parser = alert_sp.add_parser('display',
    help="Displays the results of the analysis for a given alert.")
display_alert_parser.add_argument('dir', 
    help="The directory of the alert to display")
display_alert_parser.set_defaults(func=display_alert)


def print_file_contents(uuid, submission):
    for file in submission.files:
        file_path = os.path.join(get_base_dir(), get_config()['global']['data_dir'], get_config()['global']['node'], uuid[0:3],
                                 uuid, file)
        if not os.path.exists(file_path):
            logging.warning("storage directory {0} does not exist".format(file_path))
        with open(file_path, 'rb') as f:
            print(repr(f.read()))


def display_submission(args):
    from saq.analysis.root import RootAnalysis, Submission
    from saq.json_encoding import _JSONEncoder
    from saq.submission_filter import get_submission_target_buffer

    root = RootAnalysis(storage_dir=args.storage_dir)
    root.load()

    submission = Submission(root)

    if args.submission_cmd == 'submission':
        print(get_submission_target_buffer(submission).decode('utf8'))
    elif args.submission_cmd == 'observable':
        observables_json = json.dumps(submission.observables, indent=True, sort_keys=True, cls=_JSONEncoder)
        print(f'{observables_json}'.encode('utf8', errors='backslashreplace').decode('utf8'))
    elif args.submission_cmd == 'file':
        print_file_contents(args.uuid, submission)
    elif args.submission_cmd == 'all':
        print(get_submission_target_buffer(submission).decode('utf8'))
        print_file_contents(args.uuid, submission)

    sys.exit(0)

# submission parser
submission_parser = subparsers.add_parser('submission',
    help="Submission management commands.")

# submission subparser
submission_sp = submission_parser.add_subparsers(dest='submission_cmd')

# display submission parser
display_submission_parser = submission_sp.add_parser('display',
    help="Displays the contents for all components (submission, observables, files) within a given submission as bytes objects.")
# display submission subparsers
display_submission_sp = display_submission_parser.add_subparsers(dest='submission_cmd')

display_submission_all_parser = display_submission_sp.add_parser('all',
    help="Displays the contents for all components (submission, observables, files) within a given submission as bytes objects.")
display_submission_all_parser.add_argument('storage_dir',
    help="The storage directory of the submission to display")
display_submission_all_parser.set_defaults(func=display_submission)

display_submission_submission_parser = display_submission_sp.add_parser('submission',
    help='Displays the buffer used for scanning a given file submission as a bytes object.')
display_submission_submission_parser.add_argument('storage_dir',
    help="The storage directory of the submission to display")
display_submission_submission_parser.set_defaults(func=display_submission)

display_submission_observable_parser = display_submission_sp.add_parser('observable',
    help="Displays the buffer used for scanning a given observable submission as a bytes object.")
display_submission_observable_parser.add_argument('storage_dir',
    help="The storage directory of the submission to display")
display_submission_observable_parser.set_defaults(func=display_submission)

display_submission_file_parser = display_submission_sp.add_parser('file',
    help="Displays the buffer (raw content) used for scanning a given file submission.")
display_submission_file_parser.add_argument('storage_dir',
    help="The storage directory of the submission to display")
display_submission_file_parser.set_defaults(func=display_submission)


# ============================================================================
# general utility commands
#

def submit_failed_submissions(args):
    import saq
    from saq.network_client import submit_alerts

    failed_dir = os.path.join(saq.SAQ_HOME, get_config()['network_client_ace']['failed_dir'])
    if not os.path.isdir(failed_dir):
        logging.warning("{} does not exist (no submissions available?".format(failed_dir))
        sys.exit(0)

    storage_dirs = [os.path.join(failed_dir, d) for d in os.listdir(failed_dir)]
    if not storage_dirs:
        logging.warning("nothing available to submit")
        sys.exit(0)

    logging.info("{} submissions available".format(len(storage_dirs)))

    try:
        remote_host = get_config()['network_client_ace']['remote_host']
        remote_port = get_config()['network_client_ace'].getint('remote_port')
        ssl_cert = os.path.join(saq.SAQ_HOME, get_config()['network_client_ace']['ssl_cert'])
        ssl_key = os.path.join(saq.SAQ_HOME, get_config()['network_client_ace']['ssl_key'])
        ca_path = os.path.join(saq.SAQ_HOME, get_config()['network_client_ace']['ca_path'])
        remote_hostname = get_config()['network_client_ace']['ssl_hostname']

        submit_alerts(remote_host, remote_port, ssl_cert, remote_hostname, ssl_key, ca_path, storage_dirs)

        if args.remove:
            for storage_dir in storage_dirs:
                try:
                    shutil.rmtree(storage_dir)
                    logging.info("removed {}".format(storage_dir))
                except Exception:
                    logging.error("unable to remove {}: {}".format(storage_dir))

        sys.exit(0)

    except Exception as e:
        logging.error("unable to submit: {}".format(e))
        sys.exit(1)

submit_failed_submissions_parser = subparsers.add_parser('submit-failed-submissions',
    help="Submit any failed submissions stored in the directory defined in the network_client_ace configuration section.")
submit_failed_submissions_parser.add_argument('-r', '--remove', required=False, default=False, action='store_true', dest='remove',
    help="Remove the directories that were submitted after a succuessful submissions.")
submit_failed_submissions_parser.set_defaults(func=submit_failed_submissions)

def search_archive(args):
    import saq
    from saq.database import get_db_connection

    # are we exporting into a directory?
    if args.output_dir:
        if not os.path.isdir(args.output_dir):
            try:
                os.mkdir(args.output_dir)
            except Exception as e:
                logging.error("unable to create output directory {}: {}".format(args.output_dir, e))
                sys.exit(0)

    search_items = args.search_items

    # are we reading search from standard input?
    if args.from_stdin:
        for search_item in sys.stdin:
            search_items.append(search_item.strip())

    with get_db_connection(name="email_archive") as db:
        c = db.cursor()
        query = """
SELECT
    archive_server.hostname, HEX(archive.md5)
FROM
    archive JOIN archive_server ON archive.server_id = archive_server.server_id
    {extended_from_clause}
WHERE
    {where_clauses}
"""

        where_clauses = []
        parameters = []

        extended_from_clause = "JOIN archive_search ON archive.archive_id = archive_search.archive_id"
        if args.exact:
            extended_from_clause = "JOIN archive_index ON archive.archive_id = archive_index.archive_id"

        for search_item in search_items:
            if not any([args.env_from, args.env_to, args.mail_from, args.mail_to, args.subject, args.message_id]):
                if args.exact:
                    where_clauses.append("archive_index.hash = UNHEX(MD5(%s))")
                    parameters.append(search_item)
                else:
                    where_clauses.append("archive_search.value LIKE %s")
                    parameters.append('%%{}%%'.format(search_item))
            else:
                _archive_index_template = "(archive_index.field = '{field}' AND archive_index.hash = UNHEX(MD5(%s)))"
                _archive_index_value = search_item
                _archive_search_template = "(archive_search.field = '{field}' AND archive_search.value LIKE %s)"
                _archive_search_value = '%%{}%%'.format(search_item)

                if args.env_from:
                    if args.exact:                        
                        where_clauses.append(_archive_index_template.format(field='env_from'))
                        parameters.append(_archive_index_value)
                    else:
                        where_clauses.append(_archive_search_template.format(field='env_from'))
                        parameters.append(_archive_search_value)

                if args.env_to:
                    if args.exact:
                        where_clauses.append(_archive_index_template.format(field='env_to'))
                        parameters.append(_archive_index_value)
                    else:
                        where_clauses.append(_archive_search_template.format(field='env_to'))
                        parameters.append(_archive_search_value)

                if args.mail_from:
                    if args.exact:
                        where_clauses.append(_archive_index_template.format(field='mail_from'))
                        parameters.append(_archive_index_value)
                    else:
                        where_clauses.append(_archive_search_template.format(field='mail_from'))
                        parameters.append(_archive_search_value)

                if args.mail_to:
                    if args.exact:
                        where_clauses.append(_archive_index_template.format(field='mail_to'))
                        parameters.append(_archive_index_value)
                    else:
                        where_clauses.append(_archive_search_template.format(field='mail_to'))
                        parameters.append(_archive_search_value)

                if args.subject:
                    if args.exact:
                        where_clauses.append(_archive_index_template.format(field='subject'))
                        parameters.append(_archive_index_value)
                    else:
                        where_clauses.append(_archive_search_template.format(field='subject'))
                        parameters.append(_archive_search_value)

                if args.url:
                    if args.exact:
                        where_clauses.append(_archive_index_template.format(field='url'))
                        parameters.append(_archive_index_value)
                    else:
                        where_clauses.append(_archive_search_template.format(field='url'))
                        parameters.append(_archive_search_value)

                if args.message_id:
                    if args.exact:
                        where_clauses.append(_archive_index_template.format(field='message_id'))
                        parameters.append(_archive_index_value)
                    else:
                        where_clauses.append(_archive_search_template.format(field='message_id'))
                        parameters.append(_archive_search_value)        
        
        query = query.format(extended_from_clause=extended_from_clause,
                            where_clauses=' OR '.join(where_clauses))
       # print(query)
       # print(','.join(parameters))
        c.execute(query, parameters)

        for server, md5 in c:
            # does this archive file exist?
            archive_base_dir = os.path.join(saq.DATA_DIR, get_config()['analysis_module_email_archiver']['archive_dir'])
            if args.archive_dir:
                archive_base_dir = args.archive_dir

            if not os.path.isdir(archive_base_dir):
                logging.error("archive directory {} does not exist".format(archive_base_dir))
                sys.exit(1)

            archive_path = os.path.join(archive_base_dir, server.lower(), md5.lower()[0:3],
                                        '{}.gz.gpg'.format(md5.lower()))
            if not os.path.exists(archive_path):
                logging.warning("archive email {} does not exist at {}".format(md5, archive_path))
                continue

            # are we just listing the paths?
            if not args.output_dir:
                print(archive_path)
                continue

            # are we just copying the entire encrypted email?
            if not saq.ENCRYPTION_PASSWORD:
                try:
                    shutil.copy(archive_path, args.output_dir)
                except Exception as e:
                    logging.warning("unable to copy {} to {}: {}".format(archive_path, args.output_dir, e))
                    continue

                print(os.path.join(args.output_dir, os.path.basename(archive_path)))
                continue

            # decrypt and decompress the emails
            dest_path = os.path.join(args.output_dir, md5.lower())
            with open(dest_path, 'wb') as fp:
                gpg_p = Popen(['gpg', '--no-tty', '-d', '--passphrase-fd', '0', archive_path],
                            stdout=PIPE, stderr=PIPE, stdin=PIPE)
                gunzip_p = Popen(['zcat'], stdin=gpg_p.stdout, stdout=fp)
                gpg_p.stdin.write('{}\n'.format(saq.ENCRYPTION_PASSWORD).encode())
                gpg_p.stdin.close()
                gpg_p.stdout.close()
                gunzip_p.communicate()
                gpg_p.wait()

                if gpg_p.returncode != 0:
                    logging.warning("gpg decryption failed for {} (return code {})".format(archive_path, gpg_p.returncode))
                    continue

            print(dest_path)

    sys.exit(0)

# search-archive
search_archive_parser = subparsers.add_parser('search-archive',
    help="Search the email archives.")
search_archive_parser.add_argument('--env-from', required=False, default=False, action='store_true', dest='env_from',
    help="Narrow searching to the envelope MAIL FROM field.")
search_archive_parser.add_argument('--env-to', required=False, default=False, action='store_true', dest='env_to',
    help="Narrow searching to the envelope RCPT TO field.")
search_archive_parser.add_argument('--mail-from', required=False, default=False, action='store_true', dest='mail_from',
    help="Narrow searching to the message body From field.")
search_archive_parser.add_argument('--mail-to', required=False, default=False, action='store_true', dest='mail_to',
    help="Narrow searching to the message body To field.")
search_archive_parser.add_argument('--subject', required=False, default=False, action='store_true', dest='subject',
    help="Narrow searching to the message body Subject field.")
search_archive_parser.add_argument('--message-id', required=False, default=False, action='store_true', dest='message_id',
    help="Narrow searching to the message body Message-ID field.")
search_archive_parser.add_argument('--url', required=False, default=False, action='store_true', dest='url',
    help="Narrow searching to a URL found anywhere in the email.")
search_archive_parser.add_argument('--exact', required=False, default=False, action='store_true', dest='exact',
    help="Perform an exact match (ignores options to narrow down search.) This is the fastest search.")
search_archive_parser.add_argument('--from-stdin', required=False, default=False, action='store_true', dest='from_stdin',
    help="Read search items from standard input (one per line.)")
search_archive_parser.add_argument('-d', '--output-dir', required=False, default=None, dest='output_dir',
    help="Export selected emails into the given directory. Also see the -p option in the main command.")
search_archive_parser.add_argument('-a', '--archive-dir', required=False, default=None, dest='archive_dir',
    help="Specify an alternative email archive directory. Defaults to what is specified in the analysis_module_email_archiver configuration.")
search_archive_parser.add_argument('search_items', nargs='*',
    help="One or more things to search for.  Each query will be searhed for individually.")
search_archive_parser.set_defaults(func=search_archive)

#
# remediation
#

def display_remediation_requests(args):
    #print(['ID', 'STATUS', 'TYPE', 'ACTION', 'DATE', 'TARGET']))
    row_format = "{:<8}{:<10}{:<8}{:<9}{:<20} {}"
    print(row_format.format('ID', 'STATUS', 'TYPE', 'ACTION', 'DATE', 'TARGET'))
    for r in get_db().query(Remediation).filter(Remediation.company_id == g_int(G_COMPANY_ID),
                                              Remediation.status.in_([REMEDIATION_STATUS_NEW, 
                                                                      REMEDIATION_STATUS_IN_PROGRESS]))\
                                      .order_by(Remediation.id):

        print(row_format.format(r.id, r.status, r.type, r.action, str(r.insert_date), r.key))
        #print('\t'.join(map(str, [r.id, r.status, r.type, r.action, r.insert_date, r.key])))

    sys.exit(0)

display_remediation_parser = subparsers.add_parser('display-remediation-requests', 
    help="Displays the remediation requests currently in the queue or in processing.")
display_remediation_parser.set_defaults(func=display_remediation_requests)

def clear_remediation_request(args):
    import saq
    from saq.database import Remediation
    from saq.remediation import REMEDIATION_STATUS_NEW
    from sqlalchemy import and_

    if not args.remediation_ids and not args.all:
        logging.error("no remediation ids were specified and the --all option was not used")
        sys.exit(1)
    
    clause = and_(Remediation.status == REMEDIATION_STATUS_NEW,
                  Remediation.lock == None,
                  Remediation.company_id == saq.COMPANY_ID)
    if not args.all and args.remediation_ids:
        clause = and_(clause, Remediation.id.in_(args.remediation_ids))

    logging.info("deleted {} requests".format(
        get_db().execute(Remediation.__table__.delete().where(clause)).rowcount))
    
    get_db().commit()
    sys.exit(0)

clear_remediation_request_parser = subparsers.add_parser('clear-remediation-requests',
    help="Clears one or more remediation requests.")
clear_remediation_request_parser.add_argument('-a', '--all', action='store_true', default=False,
    help="Clears all remediation requests that are not locked or have expired locks.")
clear_remediation_request_parser.add_argument('remediation_ids', nargs=argparse.REMAINDER,
    help="Zero or more remediation IDs to clear which can be obtained using the display-remediation-request command.")
clear_remediation_request_parser.set_defaults(func=clear_remediation_request)

def update_organization(args):

    # load the organization information
    config = get_config()['analysis_module_user_tagger']

    dest_file = os.path.join(get_base_dir(), config['json_path'])
    temp_file = os.path.join(get_base_dir(), '{0}.tmp'.format(config['json_path']))

    # key = userID (lowercase), value = set(tags...)
    mapping = {}

    # horrible copy-pasta (sorry)
    # load ldap settings from configuration file
    ldap_server = get_config().get('ldap', 'ldap_server')
    ldap_port = get_config().getint('ldap', 'ldap_port') or 389
    ldap_bind_user = get_config().get('ldap', 'ldap_bind_user')
    ldap_bind_password = get_config().get('ldap', 'ldap_bind_password')
    ldap_base_dn = get_config().get('ldap', 'ldap_base_dn')

    def ldap_query(query):

        from ldap3 import Server, Connection, SIMPLE, SYNC, SUBTREE, ALL, ALL_ATTRIBUTES
        import json

        try:
            with Connection(
                Server(ldap_server, port = ldap_port, get_info = ALL), 
                auto_bind = True,
                client_strategy = SYNC,
                user=ldap_bind_user,
                password=ldap_bind_password,
                authentication=SIMPLE, 
                check_names=True) as c:

                logging.debug("running ldap query for ({0})".format(query))
                c.search(ldap_base_dn, '({0})'.format(query), SUBTREE, attributes = ALL_ATTRIBUTES)

                # a little hack to move the result into json
                response = json.loads(c.response_to_json())
                result = c.result

                if len(response['entries']) < 1:
                    return None

                # XXX not sure about the 0 here, I guess only if we only looking for one thing at a time
                return response['entries'][0]['attributes']

                # look for the result with the 'type' set to 'searchResEntry'
                #for r in response['entries']:
                    #if r['type'] == 'searchResEntry':
                        # and the what we're looking for should be in here
                    #return r['attributes']

                return None

        except Exception as e:
            logging.error("unable to perform ldap query: {0}".format(str(e)))
            report_exception()
            return None

    if os.path.exists(temp_file):
        try:
            os.remove(temp_file)
        except Exception:
            logging.error("unable to remove temp file {0}".format(temp_file))
            sys.exit(1)

    def recurse_org(group_name, limit, tag, current_user_id, current_level=0):
        # add this user
        if current_user_id.lower() not in mapping:
            mapping[current_user_id.lower()] = set()

        mapping[current_user_id.lower()].add(tag)

        current_level += 1
        if limit != 'all' and current_level > int(limit):
            return

        # figure out who works for this guy
        query_results = ldap_query("cn={0}*".format(current_user_id))
        if query_results is None:
            return

        if 'directReports' in query_results:
            for direct_report in query_results['directReports']:
                # extract the userID from this thing
                m = re.search(r'CN=([^,]+),', direct_report)
                if m is None:
                    logging.warning("unable to extract direct report info from {0}".format(direct_report))

                user_id = m.group(1)
                if user_id is not None:
                    # add this guy (and possibly all his direct reports too)
                    logging.debug("adding {0} as a direct report to {1} for group {2}".format(user_id, current_user_id, group_name))
                    recurse_org(group_name, limit, tag, user_id, current_level)

    # load all the hierarchy definitions
    for section in config.keys():
        if section.startswith('group_'):
            m = re.match(r'^group_([^_]+)$', section)
            if m is None:
                logging.error("unable to parse group name from {0}".format(section))
                continue

            group_name = m.group(1)
            parent_id, limit, tag = [x.strip() for x in config[section].split(',')]
            
            recurse_org(group_name, limit, tag, parent_id)

    # write out the json
    for key in mapping.keys():
        mapping[key] = list(mapping[key])

    with open(temp_file, 'w') as fp:
        json.dump(mapping, fp)

    # finally update the production file
    try:
        shutil.move(temp_file, dest_file)
    except Exception as e:
        logging.error("unable to move {0} to {1}: {2}".fomrat(temp_file, dest_file, str(e)))

update_organization_parsers = subparsers.add_parser('update-organization',
    help="Updates the files used by the UserTaggingAnalyzer module.")
update_organization_parsers.set_defaults(func=update_organization)

def update_for_detection_observable_cache(args):
    # Get a connection to the Redis database for the observable cache.
    # Database A holds the source of truth for which observables are enabled for detection.
    # Database B is used to build the cache and uses swapdb to swap itself and Database A when finished.

    redis_connection_b = redis.Redis(
        get_config()['redis-local']['host'],
        get_config()['redis-local'].getint('port'),
        db=REDIS_DB_FOR_DETECTION_B,
        decode_responses=True,
        encoding='utf-8'
    )

    # Get the for_detection observables and store their IDs in the Redis cache
    # key = <observable_type>:<observable_value>
    # value = <observable_id>
    logging.info('Rebuilding for_detection observable cache in Redis')
    start = time.time()
    redis_connection_b.flushdb()

    count = 0
    for_detection_observables = _get_for_detect_observables_by_type()
    for observable_type in for_detection_observables:
        for observable in for_detection_observables[observable_type]:
            key = f'{observable_type}:{observable["value"]}'
            redis_connection_b.set(key, observable["id"])
            count += 1

    redis_connection_b.swapdb(REDIS_DB_FOR_DETECTION_A, REDIS_DB_FOR_DETECTION_B)
    logging.info(f'Cached {count} for_detection observables in {"%.2f" % (time.time() - start)} seconds')

update_for_detection_observable_cache_parser = subparsers.add_parser('update-for-detection-observable-cache',
    help="Updates the local Reids cache of the observables that are enabled for detection.")
update_for_detection_observable_cache_parser.set_defaults(func=update_for_detection_observable_cache)


def _get_for_detect_observables_by_type() -> dict:
    """Builds and returns a dictionary containing observables marked 'for_detection', organized by type.

        Returns:
            dict containing for_detection observables, organized by type.
            Ex. {observable_type_1: [{'id': obs1_id, 'value': obs1_value}, {id': obs2_id, 'value': obs2_value}],
                                     observable_type_2: [id': obs3_id, 'value': obs3_value}], etc.}
    """
    from saq.database import Observable

    observables_by_type = {}
    for_detect_observables = get_db().query(Observable.id, Observable.type, Observable.value).filter(
        and_(
            Observable.for_detection.is_(True),
            or_(
                Observable.expires_on.is_(None),
                datetime.datetime.utcnow() < Observable.expires_on
            )
        )
    ).all()

    for observable in for_detect_observables:
        observable_info = {'id': observable.id, 'value': observable.value.decode()}
        if observable.type in observables_by_type:
            observables_by_type[observable.type].extend([observable_info])
        else:
            observables_by_type[observable.type] = [observable_info]

    return observables_by_type


def _create_rule_dir(dir_path: str) -> bool:
    """Checks that a directory exists and creates it if not.

        Rules are built from templates configured in config sections `yara_export` and `yara_export_string_modifiers`.

        Args:
            dir_path: str containing the directory to check/create.
        Returns:
            bool containing whether the directory exists (True) or failed to create (False)
    """
    if not os.path.isdir(dir_path):
        try:
            os.makedirs(dir_path)
        except Exception as e:
            logging.error(f"unable to create directory {dir_path}: {e}")
            return False

    return True


def _export_yara_rules(export_dir: str, for_detect_observables: dict) -> None:
    """Exports a given dictionary of observables into Yara rules.

        Rules are built from templates configured in config sections `yara_export` and `yara_export_string_modifiers`.

        Args:
            export_dir: A str containing the directory to output the rules to.
            for_detect_observables: A dictionary of observables to be exported to yara rules.
                                    Note: In order to work, this dictionary must follow the below structure:
                                    {observable_type_1: [{'id': obs1_id, 'value': obs1_value}, {id': obs2_id, 'value': obs2_value}],
                                     observable_type_2: [id': obs3_id, 'value': obs3_value}], etc.}
    """

    #
    # Func-specific Imports
    #
    from datetime import date
    from saq.environment import get_analyst_data_dir
    import yara
    yara.set_config(max_strings_per_rule=int(get_config()['yara_export']['max_strings_per_rule']))

    #
    # Setup Constants and Load Config
    #

    # Load the mapping from observable type to the string modifiers to use
    string_modifiers = collections.defaultdict(lambda: get_config()['yara_export_string_modifiers']['default'])
    for observable_type in get_config()['yara_export_string_modifiers']:
        if observable_type == 'default':
            continue

        string_modifiers[observable_type] = get_config()['yara_export_string_modifiers'][observable_type]
        logging.debug(f"using string modifiers {string_modifiers[observable_type]} for {observable_type}")

    # What is the minimum length an observable value can be to be put into a yara rule?
    export_minimum_length = get_config()['yara_export'].getint('export_minimum_length')

    # What observable types should be exported to yara?
    observable_types = [_.strip() for _ in get_config()['yara_export']['export_list'].split(',')]

    #
    # Define helper functions
    #
    def format_yara_string(s):
        return s.replace('\\', '\\\\').replace('"', '\\"').replace("\n", "").replace("\r", "")

    def format_yara_json_string(s):
        return s.replace('\\', '\\\\\\\\').replace('"', '\\"').replace("\n", "").replace("\r", "")

    def format_yara_default_string():
        # add yara rule for observable
        yara_value = format_yara_string(obs_value)
        mods = string_modifiers[observable_type.lower()]
        observable_string_data.write(f'          $obs_{obs_id} = "{yara_value}" {mods}\n')

        # skip json value if it is the same as the regular value
        yara_json_value = format_yara_json_string(obs_value)
        if yara_value == yara_json_value:
            return

        # add json value which will hit in json files
        observable_string_data.write(f'          $obs_json_{obs_id} = "{yara_json_value}" {mods}\n')

    #
    # Check/create directory for rule output
    #
    if not _create_rule_dir(export_dir):
        sys.exit(1)

    #
    # Loop through configured types and create a respective rule file for each
    #
    for observable_type in observable_types:
        if observable_type not in for_detect_observables:
            logging.info(f"no observables of type {observable_type} available for export")
            continue
        logging.info(f"exporting observable type {observable_type}")

        # Check if there's a template for current observable type
        template_path = os.path.join(get_analyst_data_dir(), get_config()['yara_export']['export_template_dir'], f'{observable_type}.template')
        if not os.path.exists(template_path):
            template_path = os.path.join(get_analyst_data_dir(), get_config()['yara_export']['export_template_dir'], 'default.template')
        logging.debug(f"using template {template_path} for {observable_type}")

        # Load and setup the template we're going to be using
        with open(template_path, 'r') as fp:
            template = fp.read()

        template_rule_name = (re.sub(r'[^a-zA-Z0-9_]', '', observable_type))
        template = template.replace('TEMPLATE_RULE_NAME', template_rule_name)
        template = template.replace('TEMPLATE_TAGS', template_rule_name)

        today = date.today()
        template = template.replace('TEMPLATE_DATE_STRING', today.strftime("%m/%d/%Y"))

        #
        # Loop through all observables of current type and create a string for each
        #

        # Buffer for entire rule file
        rule_data = io.StringIO()
        count = 0
        skip_count = 0
        for observable in for_detect_observables[observable_type]:
            obs_id = observable['id']
            obs_value = observable['value']

            if len(obs_value) < export_minimum_length:
                skip_count += 1
                continue

            # Buffer for respective observable string
            observable_string_data = io.StringIO()
            format_yara_default_string()

            # Test individual string compiles into yara
            test_rule = template.replace('TEMPLATE_STRINGS', observable_string_data.getvalue())
            try:
                yara.compile(source=test_rule)
            except Exception as e:
                logging.error(f"observable type {observable_type} id {obs_id} value {obs_value} invalid for yara: {e}")
                print(test_rule)
                skip_count += 1
                continue

            count += 1
            rule_data.write(observable_string_data.getvalue())

        if count == 0:
            logging.info(f"no observables of type {observable_type} available for export")
            continue

        #
        # Test and write rule file for current observable type
        #
        
        # Test that entire rule compiles
        rule = template.replace('TEMPLATE_STRINGS', rule_data.getvalue())
        try:
            yara.compile(source=rule)
        except Exception as e:
            logging.error(f"unable to compile rules for {observable_type}: {e}")
            print(rule)
            continue

        # Make sure rule has changed before writing
        output_file = os.path.join(export_dir, f"{observable_type}.yar")
        if os.path.exists(output_file):
            with open(output_file, 'r') as fp:
                existing_rule = fp.read()

            if existing_rule == rule:
                logging.info(f"no changes detected for {observable_type}")
                continue

        # Write the rule to file
        with open(output_file, 'w') as fp:
            fp.write(rule)

        logging.info(f"exported {count} skipped {skip_count} observables of type {observable_type} to {output_file}")
        if count == 0:
            logging.warning(f"no strings were exported for {observable_type}, removing {output_file}")
            try:
                os.remove(output_file)
            except Exception as e:
                logging.error(f"unable to remove {output_file}: {e}")


def _export_splunk_observables(config: str, collection: str, for_detect_observables: dict, dry_run: bool) -> None:
    """Exports a given dictionary of observables to a Splunk KVStore Collection.

        Args:
            collection: A str containing the KVStore collection to export to.
            for_detect_observables: A dictionary of observables to be exported to Splunk.
                                    Note: In order to work, this dictionary must follow the below structure:
                                    {observable_type_1: [{'id': obs1_id, 'value': obs1_value}, {id': obs2_id, 'value': obs2_value}],
                                     observable_type_2: [id': obs3_id, 'value': obs3_value}], etc.}
            dry_run: A bool specifying whether the collection should actually be updated; if False, the items to be deleted/uploaded
                        will be logged to the console instead of actually ran.
    """
    max_export = get_config().getint(config, 'max_export')  # the maximum number of observables to export in each request
    from saq.splunk import SplunkClient
    splunk = SplunkClient(
        get_config()[config]['api'],
        user_context = get_config()[config]['user_context'],
        app = get_config()[config]['app'],
    )

    # What observable types should be exported to Splunk?
    observable_types = [_.strip() for _ in get_config()[config]['export_list'].split(',')]

    # Gather export data
    export_data = []  # List of dicts for each observable to be exported
    export_ids = set()  # List of IDs for each observable to be exported (used to delete stale observables in KVStore)
    for observable_type in observable_types:
        if observable_type not in for_detect_observables:
            logging.info(f"no observables of type {observable_type} available for export")
            continue

        count = 0  # counter of how many observables to export of current type
        for observable in for_detect_observables[observable_type]:
            escaped_observable_value = observable['value'].lower().replace('*', '\\*')
            export_data.append({"_key": str(observable['id']),
                                "id": observable['id'],
                                "type": observable_type,
                                "value": f"*{escaped_observable_value}*"})

            export_ids.add(observable['id'])
            count += 1

        logging.info(f"{count} observables of type {observable_type} available for export")

    if not export_ids:
        if dry_run:
            logging.info("DRYRUN: No observables to export; All observables will be deleted from collection!")
        else:
            logging.info("No observables to export; Deleting all existing data from collection (may already be empty)")
            deleted = splunk.delete_all_from_kvstore(collection)
            if not deleted:
                logging.error("Data could not be deleted from Splunk KVStore collection!")

        return

    # Find and delete observables in lookup table that are no longer enabled for detection
    current_observables = splunk.get_all_from_kvstore(collection)
    if current_observables:
        current_ids = set([obs['id'] for obs in current_observables])
        if current_ids == export_ids:
            logging.info("Current KVStore collection matches export data; no changes necessary.")
            return

        if current_ids.isdisjoint(export_ids):
            if dry_run:
                logging.info("DRYRUN: All current observables are stale; Deleting all stale data from collection!")
            else:
                logging.info("All current observables are stale; Deleting all stale data from collection")
                deleted = splunk.delete_all_from_kvstore(collection)
                if not deleted:
                    logging.error("Stale data could not be deleted from Splunk KVStore collection!")
                    return

        else:
            stale_ids = current_ids.difference(export_ids)
            if stale_ids and not dry_run:
                for _id in stale_ids:
                    deleted = splunk.delete_from_kvstore_by_id(collection, _id)
                    if not deleted:
                        logging.error(f"Observable {_id} could not be deleted from Splunk KVStore collection!")
                        return

                logging.info(f"Deleted {len(stale_ids)} stale observables from Splunk KVStore collection '{collection}'!")

            else:
                logging.info(f"DRYRUN: All observables in list {stale_ids} will be deleted from collection! ")

            if current_ids.intersection(export_ids) == export_ids:
                logging.info("Updated KVStore collection matches export data; no further changes necessary.")
                return

    if dry_run:
        logging.info(f"DRYRUN: {len(export_ids)} observables will be exported to "
                     f"Splunk KVStore collection '{collection}' in chunks of {max_export}!")
        logging.info(f"DRYRUN: (Example) First 10 observables to be exported (don't worry about double backlashes): {export_data[:10]}")

    else:
        # Batch save new/existing observables to KVStore collection
        # We're keying on observable ID so we don't need to worry about duplicates
        logging.info(f"Exporting {len(export_ids)} observables to Splunk KVStore collection '{collection}' in chunks of {max_export}!")
        chunks = [export_data[x:x+max_export] for x in range(0, len(export_data), max_export)]
        for chunk in chunks:
            added_items = splunk.save_to_kvstore(collection, chunk)
            if not added_items:
                logging.error("Observables could not be added to Splunk KVStore collection!")

        logging.info(f"Added observables to Splunk KVStore collection '{collection}'!")


def export_for_detect_observables(args):
    for_detect_observables = _get_for_detect_observables_by_type()

    for export_request in args.type:
        if export_request[0] == 'yara':
            _export_yara_rules(export_request[1], for_detect_observables)
        elif export_request[0] == 'splunk':
            _export_splunk_observables(args.splunk_config, export_request[1], for_detect_observables, args.dry_run)
        else:
            print('Invalid export option!')

    sys.exit(0)


export_for_detect_parser = subparsers.add_parser('export-for-detect-observables',
                                                 help="Exports observables marked for_detection into configured formatting")
export_for_detect_parser.add_argument('--type', action='append', nargs=2, type=str, required=True,
                                                 help="{type of export to perform} {output directory/KVStore Collection/etc.}")
export_for_detect_parser.add_argument('--dry-run', action='store_true', required=False,
                                                 help="(Optional) Only available for Splunk export. Outputs what will be deleted/uploaded.")
export_for_detect_parser.add_argument(
    '--splunk-config',
    type=str,
    default='splunk_export',
    help = 'Name of the section in the config to use for the export settings. Defaults to splunk_export',
)
export_for_detect_parser.set_defaults(func=export_for_detect_observables)

def cleanup_email_archive(args):
    from saq.email import maintain_archive
    maintain_archive(verbose=True)
    sys.exit(0)

cleanup_email_archive_parser = subparsers.add_parser('cleanup-email-archive',
    help="Removes emails from the archive that have expired.")
cleanup_email_archive_parser.set_defaults(func=cleanup_email_archive)

# ============================================================================
# company management
#

def list_companies(args):
    from saq.database import get_db_connection

    with get_db_connection() as db:
        c = db.cursor()
        c.execute("SELECT id, name FROM company ORDER BY name")
        print()
        print("ID\tNAME")
        for company_id, company_name in c:
            print("{}\t{}".format(company_id, company_name))

    print()
    print("use ./saq add-company and ./saq delete-company to manage companies")
    print()
    sys.exit(0)

list_companies_parser = subparsers.add_parser('list-companies',
    help="Lists the available companies and their IDs.")
list_companies_parser.set_defaults(func=list_companies)

def add_company(args):
    from saq.database import get_db_connection

    with get_db_connection() as db:
        c = db.cursor()
        c.execute("INSERT INTO company ( `id`, `name` ) VALUES ( %s, %s )", (args.company_id, args.company_name))
        db.commit()
        logging.info("company added")

    sys.exit(0)

add_companies_parser = subparsers.add_parser('add-company',
    help="Adds a new company entry.")
add_companies_parser.add_argument('company_id', type=int, help="The ID of the new company (a number that is not already being used as an ID.)")
add_companies_parser.add_argument('company_name', help="The name of the new company.")
add_companies_parser.set_defaults(func=add_company)

def delete_company(args):
    import saq
    from saq.constants import INSTANCE_TYPE_PRODUCTION
    from saq.database import get_db_connection

    print("***************************************************************")
    print("Deleting a company will delete all associated EVENTS and ALERTS.")
    confirm = input("Are you SURE? (Y/n)")
    if confirm != 'Y':
        print("Action not taken.")
        sys.exit(0)

    confirm = input("Are you DAMN SURE? Seriously. If you're wrong it will be a disaster. (Y/n)")
    if confirm != 'Y':
        print("Action not taken.")
        sys.exit(0)

    # see if we are on the production system
    if saq.INSTANCE_TYPE == INSTANCE_TYPE_PRODUCTION:
        confirm = input("You are in a PRODUCTION SERVER. Are you SURE you know what you are doing? Type YES if you are sure.")
        if confirm != 'YES':
            print("Action not taken. Pay attention to what you're doing please.")
            sys.exit(0)

    with get_db_connection() as db:
        c = db.cursor()
        c.execute("DELETE FROM company WHERE `name` = %s", (args.company_name,))
        db.commit()
        logging.info("company deleted")

    sys.exit(0)

delete_companies_parser = subparsers.add_parser('delete-company',
    help="Deletes a given company and all associated events and alerts.")
delete_companies_parser.add_argument('company_name', help="The name of the company to delete.")
delete_companies_parser.set_defaults(func=delete_company)

# ============================================================================
# testing utilities
#

def test_proxies(args):
    proxy_configs = g_obj(G_OTHER_PROXIES).value.copy()
    proxy_configs['GLOBAL'] = get_config()['proxy']

    for proxy_name, proxy_config in proxy_configs.items():
        print("testing proxy {} ({})".format(proxy_name, proxy_config))
        session = requests.session()
        session.proxies = proxy_config
        response = session.request('GET', args.url,
                                   timeout=20,
                                   allow_redirects=True,
                                   verify=False)

        print("result: ({}) - {}".format(response.status_code, response.reason))

    sys.exit(0)

test_proxies_parser = subparsers.add_parser('test-proxies',
    help="Test the configured proxies to make sure ACE can use them.")
test_proxies_parser.add_argument('url', help="A sample URL to attempt to download through each proxy.")
test_proxies_parser.set_defaults(func=test_proxies)

# XXX replace this with calls to the engine code
def verify_modules(args):
    """Executes verify_environment() on all modules that are enabled."""
    # we run the same code the engines run to load the modules
    # COPY-PASTA!!
    import importlib

    analysis_module_sections = [section for section in get_config().sections() if section.startswith('analysis_module_')]
    for section in sorted(analysis_module_sections):

        # is this module disabled globally?
        # modules that are disable globally are not used anywhere
        if not get_config().getboolean(section, 'enabled'):
            logging.debug("analysis module {} disabled (globally)".format(section))
            continue

        logging.debug("verifying analysis module from {}".format(section))
        module_name = get_config().get(section, 'module')
        try:
            _module = importlib.import_module(module_name)
        except Exception:
            logging.error("unable to import module {}".format(module_name, ))
            traceback.print_exc()
            continue

        class_name = get_config().get(section, 'class')
        try:
            module_class = getattr(_module, class_name)
        except AttributeError:
            logging.error("class {} does not exist in module {} in analysis module {}".format(
                          class_name, module_name, section))
            traceback.print_exc()
            continue

        try:
            analysis_module = module_class(section)
        except Exception as e:
            logging.error("unable to load analysis module {}: {}".format(section, e))
            traceback.print_exc()
            continue

        # make sure the module has everything it needs
        try:
            analysis_module.verify_environment()
        except Exception as e:
            logging.error("analysis module {} failed environment verification: {}".format(analysis_module, e))
            traceback.print_exc()
            continue

        logging.info("analysis module {} verification OK".format(section))
    
verify_modules_parsers = subparsers.add_parser('verify-modules',
    help="Executes verify_environment() on all modules that are enabled.")
verify_modules_parsers.set_defaults(func=verify_modules)

# ============================================================================
# encryption/decryption utilities
#

encryption_parser = subparsers.add_parser('encryption', aliases=['enc'],
    help="Encryption management commands.")
encryption_sp = encryption_parser.add_subparsers(dest='enc_cmd')

def set_encryption_password(args):
    from saq.crypto import set_encryption_password, get_aes_key, InvalidPasswordError, encryption_key_set
    while True:
        current_password = None
        if encryption_key_set() and not args.overwrite:
            current_password = getpass.getpass("Enter the CURRENT encryption password:")
            try:
                get_aes_key(current_password)
            except InvalidPasswordError:
                print("ERROR: invalid password")
                print("if you can't remember you can use the --overwrite option")
                print("but then you won't be able to access anything you've already encrypted")
                continue
            
        if args.password is None:
            password = getpass.getpass("enter the new encryption password:")
            password_2 = getpass.getpass("enter the new encryption password again for verification:")
        else:
            password = password_2 = args.password

        if password != password_2:
            logging.error("passwords do not match")
            continue

        break

    key = None
    if args.key:
        while True:
            key = getpass.getpass("enter the primary encryption key password:")
            key_2 = getpass.getpass("enter the primary encryption key password again for verification:")

            if key != key_2:
                logging.error("passwords do not match")
                continue

            break

        from Crypto.Hash import SHA256
        h = SHA256.new()
        h.update(key.encode())
        key = h.digest()

    set_encryption_password(password, old_password=current_password, key=key)
    sys.exit(0)

set_encryption_password_parser = subparsers.add_parser('set-encryption-password',
    help="Sets the password used to encrypt and decrypt archived emails.")
set_encryption_password_parser.add_argument('-o', '--overwrite', default=False, action='store_true',
    help="Overwrites an existing password without prompting.")
set_encryption_password_parser.add_argument('-k', '--key', default=False, action='store_true',
    help="Use the sha256 hash of a string as the primary encryption key. The input is prompted for.")
set_encryption_password_parser.set_defaults(func=set_encryption_password)

set_encryption_password_parser = encryption_sp.add_parser('set',
    help="Sets the password used to encrypt and decrypt archived emails.")
set_encryption_password_parser.add_argument('-o', '--overwrite', default=False, action='store_true',
    help="Overwrites an existing password without prompting.")
set_encryption_password_parser.add_argument('-k', '--key', default=False, action='store_true',
    help="Use the sha256 hash of a string as the primary encryption key. The input is prompted for.")
set_encryption_password_parser.add_argument('-p', '--password',
    help="Use this for the password instead of prompting. Don't use this option unless you have a good reason to.")
set_encryption_password_parser.set_defaults(func=set_encryption_password)

def test_encryption_password(args):
    import saq.crypto
    if args.password is None:
        password = getpass.getpass("Enter the decryption password:")
    else:
        password = args.password

    try:
        saq.crypto.get_aes_key(password)
        logging.info("password OK")
    except saq.crypto.InvalidPasswordError:
        logging.error("invalid password")
        sys.exit(1)

    sys.exit(0)

test_encryption_password_parser = encryption_sp.add_parser('test',
    help="Tests the given password to see if it matches what is currently set as the encryption password.")
test_encryption_password_parser.add_argument('-p', '--password',
        help="Provide the password on the command line. Only recommended for automation purposes.")
test_encryption_password_parser.set_defaults(func=test_encryption_password)

def encrypt_file(args):
    from saq.crypto import encrypt

    password = None
    if args.password:
        password = args.password
    elif args.prompt:
        password = getpass.getpass("enter encryption password: ")
        verify = getpass.getpass("verify encryption password: ")
        if password != verify:
            print("passwords do not match")
            sys.exit(1)

    encrypt(args.source_path, args.target_path, password=password)
    sys.exit(0)

encrypt_file_parser = encryption_sp.add_parser('encrypt',
    help="Encrypts the given file with the password set with set-encryption-password.")
encrypt_file_parser.add_argument('source_path', help="The file to encrypt from.")
encrypt_file_parser.add_argument('target_path', help="The file to saved the decrypted data to.")
encrypt_file_parser.add_argument('--password', help="Use the given password to encrypt the file.")
encrypt_file_parser.add_argument('--prompt', action='store_true', default=False, 
    help="Prompt for the password to use to encrypt the file.")
encrypt_file_parser.set_defaults(func=encrypt_file)

def decrypt_file(args):
    from saq.crypto import decrypt

    password = None
    if args.password:
        password = args.password
    elif args.prompt:
        password = getpass.getpass("enter decryption password: ")

    decrypt(args.source_path, args.target_path, password=password)
    sys.exit(0)

decrypt_file_parser = encryption_sp.add_parser('decrypt',
    help="Decrypts the given file with the password set with set-decryption-password.")
decrypt_file_parser.add_argument('source_path', help="The file to decrypt from.")
decrypt_file_parser.add_argument('target_path', help="The file to saved the decrypted data to.")
decrypt_file_parser.add_argument('--password', help="Use the given password to decrypt the file.")
decrypt_file_parser.add_argument('--prompt', help="Prompt for the password to use to decrypt the file.")
decrypt_file_parser.set_defaults(func=decrypt_file)

config_encryption_parser = encryption_sp.add_parser('config',
    help="Configuration encryption management commands.")
config_encryption_sp = config_encryption_parser.add_subparsers(dest='config_enc_cmd')

def set_encrypted_password(args):
    if g_obj(G_ENCRYPTION_KEY).value is None:
        logging.error("missing encryption password (use the -p option or start the ecs service)\n")
        sys.exit(1)

    while True:
        password = getpass.getpass("Enter the data to be encrypted and stored:")
        password_2 = getpass.getpass("Re-enter the value for verification:")

        if password != password_2:
            logging.error("passwords do not match, try again")
            continue

        break

    from saq.configuration.parser import encrypt_password
    encrypt_password(args.key, password)
    sys.exit(0)

set_encrypted_password_parser = config_encryption_sp.add_parser('set',
    help="Set a password in the system, storing the value encrypted in the database. You will be prompted for the password.")
set_encrypted_password_parser.add_argument('key',
    help="The key (name) of the password.")
set_encrypted_password_parser.set_defaults(func=set_encrypted_password)

def list_encrypted_passwords(args):
    from saq.configuration.parser import export_encrypted_passwords
    encrypted_passwords = export_encrypted_passwords()
    if not encrypted_passwords:
        print("no passwords have been encrypted")
        sys.exit(0)

    for key, value in encrypted_passwords.items():
        if value is None:
            value = "<encrypted>"

        print(f"{key} = {value}")

    sys.exit(0)

list_encrypted_passwords_parser = subparsers.add_parser('list-encrypted-passwords',
    help="Lists the encrypted passwords.")
list_encrypted_passwords_parser.set_defaults(func=list_encrypted_passwords)

list_encrypted_passwords_parser = config_encryption_sp.add_parser('list',
    help="Lists the encrypted passwords.")
list_encrypted_passwords_parser.set_defaults(func=list_encrypted_passwords)

def delete_encrypted_password(args):
    from saq.configuration.parser import delete_password
    if delete_password(args.key):
        print("password deleted")
    else:
        print("ERROR: unable to delete password")

    sys.exit(0)

delete_encrypted_password_parser = subparsers.add_parser('delete-encrypted-password',
    help="Deletes a given encrypted password.")
delete_encrypted_password_parser.add_argument('key',
    help="The name of the password to delete.")
delete_encrypted_password_parser.set_defaults(func=delete_encrypted_password)

delete_encrypted_password_parser = config_encryption_sp.add_parser('delete',
    help="Deletes a given encrypted password.")
delete_encrypted_password_parser.add_argument('key',
    help="The name of the password to delete.")
delete_encrypted_password_parser.set_defaults(func=delete_encrypted_password)

def export_encrypted_passwords(args):
    import saq.configuration.parser

    if args.file == '-':
        fp = sys.stdout
    else:
        fp = open(args.file, 'w')

    json.dump(saq.configuration.parser.export_encrypted_passwords(), fp)
    fp.write('\n')
    fp.close()
    sys.exit(0)

export_encrypted_passwords_parser = config_encryption_sp.add_parser('export',
    help="Exports the encrypted passwords to JSON.")
export_encrypted_passwords_parser.add_argument('file',
    help="""The name of the file to store the JSON. 
            Use a file name of - to export to stdout.""")
export_encrypted_passwords_parser.set_defaults(func=export_encrypted_passwords)

def import_encrypted_passwords(args):
    import saq.configuration.parser

    if args.file == '-':
        fp = sys.stdin
    else:
        fp = open(args.file, 'r')

    saq.configuration.parser.import_encrypted_passwords(json.load(fp))
    fp.close()
    sys.exit(0)

import_encrypted_passwords_parser = config_encryption_sp.add_parser('import',
    help="Imports the encrypted passwords JSON generated by the export command.")
import_encrypted_passwords_parser.add_argument('file',
    help="""The name of the JSON export. 
            Use a file name of - to import from stdin.""")
import_encrypted_passwords_parser.set_defaults(func=import_encrypted_passwords)

def remove_bro_http_whitelist(args):
    removed_line = False
    src_path = os.path.join(get_base_dir(), 'bro', 'http.whitelist')
    tmp_path = os.path.join(get_base_dir(), 'bro', 'http.whitelist.tmp')

    with open(src_path, 'r') as fp_in:
        with open(tmp_path, 'a') as fp_out:
            for line in fp_in:
                if line.startswith(args.cidr):
                    logging.info("removed {}".format(line.strip()))
                    removed_line = True
                else:
                    fp_out.write(line)

    if removed_line:
        shutil.copy(tmp_path, src_path)
    
    os.remove(tmp_path)
    sys.exit(0)

remove_bro_http_whitelist_parser = subparsers.add_parser('remove-bro-http-whitelist',
    help="Adds the given CIDR and description as a whitelist item to the bro HTTP whitelist.")
remove_bro_http_whitelist_parser.add_argument('cidr', help="The network CIDR to remove from the whitelist.")
remove_bro_http_whitelist_parser.set_defaults(func=remove_bro_http_whitelist)

def list_available_modules(args):
    from saq.engine.module_loader import load_module

    result = {}

    for section in get_config().keys():
        if not section.startswith('analysis_module_'):
            continue

        if section in get_config()['disabled_modules'] and get_config()['disabled_modules'].getboolean(section):
            continue

        result[section] = load_module(section)

    for key in sorted(result.keys()):
        print('{: <35}{}'.format(key[len('analysis_module_'):], result[key].__doc__ if result[key].__doc__ is not None else ''))

    sys.exit(0)

list_available_modules_parser = subparsers.add_parser('list-available-modules',
    help="Lists the modules available in ACE.")
list_available_modules_parser.set_defaults(func=list_available_modules)

def config(args):

    def matches_param(s, k=None):
        if not args.settings:
            return True

        for spec in args.settings:
            section_spec, key_spec = spec.split('.')
            #print("testing {} == {} {} == {}".format(section_spec, s, key_spec, k))
            if ( section_spec == '*' or section_spec == s ) and ( k is None or key_spec == '*' or key_spec == k ):
                return True

        return False

    for section in list(get_config().keys()):
        if section == 'DEFAULT':
            continue

        if not matches_param(section):
            continue

        if not args.value_only:
            print('[{}]'.format(section))
        
        for key in get_config()[section].keys():
            if not matches_param(section, key):
                continue

            if args.value_only:
                print(get_config()[section][key])
            else:
                print('{} = {}'.format(key, get_config()[section][key]))

        if not args.value_only:
            print()

    sys.exit(0)

config_parser = subparsers.add_parser('config',
    help="Queries the ACE configuration.")
config_parser.add_argument('-v', '--value', action='store_true', default=False, dest='value_only',
    help="Just print the values of the selected configuration items.")
config_parser.add_argument('settings', nargs="*",
    help="Zero or more configuration items to display in the format section.key.")
config_parser.set_defaults(func=config)

def display_workload(args):
    from saq.database import get_db_connection

    with get_db_connection() as db:
        c = db.cursor()
        c.execute("SELECT analysis_mode, COUNT(*) FROM workload WHERE company_id = %s GROUP BY analysis_mode ORDER BY analysis_mode", (g_int(G_COMPANY_ID),))
        print(f" -- WORKLOAD ({g(G_COMPANY_NAME)}) --")
        print("{: <15}{}".format('MODE', 'COUNT'))
        for analysis_mode, count in c:
            print("{: <15}{}".format(analysis_mode, count))

        db.commit()
        print()
        print(f" -- DELAYED WORKLOAD ({g(G_SAQ_NODE)})--")
        c.execute("SELECT storage_dir, analysis_module, COUNT(*) FROM delayed_analysis JOIN nodes ON delayed_analysis.node_id = nodes.id WHERE nodes.name = %s GROUP BY storage_dir, analysis_module", (g(G_SAQ_NODE),))
        print("{: <36} {: <20} {}".format('UUID', 'MODULE', 'COUNT'))
        for storage_dir, analysis_module, count in c:
            storage_dir = os.path.basename(storage_dir)
            analysis_module = analysis_module[len('analysis_module_'):]
            print("{: <36} {: <20} {}".format(storage_dir, analysis_module, count))

        db.commit()
        print()
        print(f" -- LOCKS ({g(G_SAQ_NODE)}) --")
        c.execute("SELECT uuid, lock_uuid, lock_time, lock_owner FROM locks WHERE lock_owner LIKE CONCAT(%s, '-%%') ORDER BY lock_time", (g(G_SAQ_NODE),))
        print("{: <36} {: <36} {: <19} {}".format('UUID', 'LOCK', 'TIME', 'OWNER'))
        for _uuid, lock_uuid, lock_time, lock_owner in c:
            print("{: <36} {: <36} {: <19} {}".format(_uuid, lock_uuid, str(lock_time), lock_owner))
        db.commit()

    sys.exit(0)

display_workload_parser = subparsers.add_parser('display-workload',
    help="Displays the current ACE workload.")
display_workload_parser.set_defaults(func=display_workload)

def build_suricata_db(args):
    from saq.modules.snort import build_signature_db
    signature_path = get_config()['analysis_module_snort_signature_analysis_v1']['signature_path']
    if args.signature_path:
        signature_path = args.signature_path

    try:
        build_signature_db(signature_path)
    except Exception as e:
        logging.error(f"unable to rebuild suricata db: {e}")
        sys.exit(1)

    sys.exit(0)

build_suricata_db_parser = subparsers.add_parser('build-suricata-db',
    help="Builds the fast suricata signature lookup redis database.")
build_suricata_db_parser.add_argument('--signature-path', default=None,
    help="""Path to the suricata signature file to load. 
            Defaults to the signature_path option in the analysis_module_snort_signature_analysis_v1 configuration section.""")
build_suricata_db_parser.set_defaults(func=build_suricata_db)

#
# events
#

def close_event(args):
    from saq.background_exec import add_background_task, BG_TASK_CLOSE_EVENT
    from saq.database import Event, EventStatus
    from saq.error import report_exception
    from saq.event import get_auto_close_events

    # are we automatically closing events?
    if args.auto:
        event_ids = get_auto_close_events()
    else:
        # otherwise we're closing an event manually
        event_ids = args.event_id

    # Set the event status to the configured closed status
    for event_id in event_ids:
        if not event_id:
            continue

        try:
            logging.info(f"closing event {event_id}")
            event = get_db().query(Event).filter(Event.id == event_id).one()
            config_closed_status = get_config().get('events', 'closed_status', fallback='CLOSED')
            closed_status = get_db().query(EventStatus).filter(EventStatus.value == config_closed_status).one()
            event.status = closed_status
            print("event closed")
        except Exception as e:
            print(f"unable to close event: {e}")
            report_exception()

    if args.dry_run:
        get_db().rollback()
    else:
        get_db().commit()
        for event_id in event_ids:
            add_background_task(BG_TASK_CLOSE_EVENT, event_id)

    sys.exit(0)

event_parser = subparsers.add_parser('event')
event_sp = event_parser.add_subparsers(dest='event_cmd')

close_event_parser = event_sp.add_parser('close',
    help="Manually or automatically close an event.")
close_event_parser.add_argument("-a", "--auto", action="store_true", default=False,
    help="Automatically close any events that are set to auto close and have expired.")
close_event_parser.add_argument("-i", "--event-id", type=int, action="append", default=[],
    help="The ID of an event to close. This can be specified multiple times.")
close_event_parser.add_argument("--dry-run", action="store_true", default=False,
    help="Don't actually close the event.")
close_event_parser.set_defaults(func=close_event)

def debug(args):
    # use this when you need it
    sys.exit(0)

debug_parser = subparsers.add_parser('debug')
debug_parser.set_defaults(func=debug)

def generate_api_key(args):
    from saq.util import sha256_str
    api_key = str(uuid.uuid4())
    print(f"api_key = {api_key}")
    print(f"api_key_sha256 = {sha256_str(api_key)}")
    sys.exit(0)

generate_api_key_parser = subparsers.add_parser('generate-api-key')
generate_api_key_parser.set_defaults(func=generate_api_key)

if __name__ == '__main__':

    # there is no reason to run anything as root
    if os.geteuid() == 0:
        print("do not run ace as root please")
        sys.exit(1)

    # parse the command line arguments
    args = parser.parse_args()

    encryption_password_plaintext = None
    if args.provide_decryption_password:
        encryption_password_plaintext = getpass.getpass(prompt="Enter the encryption password:")

    # initialize saq
    initialize_environment(
        saq_home=args.saq_home,
        log_level=args.log_level,
        config_paths=args.config_paths,
        logging_config_path=args.logging_config_path,
        relative_dir=args.relative_dir,
        encryption_password_plaintext=encryption_password_plaintext,
        skip_initialize_automation_user=args.skip_initialize_automation_user)

    if args.debug_on_error:
        def info(type, value, tb):
           if hasattr(sys, 'ps1') or not sys.stderr.isatty() or type != AssertionError:
              # we are in interactive mode or we don't have a tty-like
              # device, so we call the default hook
              sys.__excepthook__(type, value, tb)
           else:
              import traceback
              import pdb
              # we are NOT in interactive mode, print the exception...
              traceback.print_exception(type, value, tb)
              print()
              # ...then start the debugger in post-mortem mode.
              pdb.pm()

        sys.excepthook = info

    # call the handler for the given command
    args.func(args)
